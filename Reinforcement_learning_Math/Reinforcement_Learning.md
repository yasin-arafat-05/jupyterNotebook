

### рзз. MDP (Markov Decision Process):  
RL-ржПрж░ ржнрж┐рждрзНрждрж┐ рж╣рж▓рзЛ MDPред ржПржЦрж╛ржирзЗ ржЖржорж░рж╛ рж╢рж┐ржЦрзЗржЫрж┐:  
- Environment ржХрзЗ state, action, transition probability, reward ржжрж┐рзЯрзЗ ржоржбрзЗрж▓ ржХрж░рж╛ ржпрж╛рзЯред  
- Agent ржПрж░ ржЧрзЛрж▓ рж╣рж▓рзЛ long-term reward (return) ржорзНржпрж╛ржХрзНрж╕рж┐ржорж╛ржЗржЬ ржХрж░рж╛ред  
- ржХрж┐ржирзНрждрзБ MDP рж╢рзБржзрзБ ржлрж░рзНржорж╛рж▓рж╛ржЗржЬрзЗрж╢ржи, рж╕ржорж╛ржзрж╛ржи (solve) ржХрж░рж╛рж░ ржкржжрзНржзрждрж┐ ржирзЯред  


### рзи. Bellman Equation:  
MDP-ржХрзЗ solve ржХрж░рждрзЗ ржЧрзЗрж▓рзЗ ржЧржгрж┐рждрзЗрж░ ржЯрзБрж▓рж╕ ржжрж░ржХрж╛рж░ред Bellman Equation рж╣рж▓рзЛ рж╕рзЗржЗ ржЯрзБрж▓:  
- ржПржЯрж┐ value function (рж╕рзНржЯрзЗржЯ/ржЕрзНржпрж╛ржХрж╢ржирзЗрж░ ржорзВрж▓рзНржп) ржХрзЗ recursive ржнрж╛ржмрзЗ ржбрж┐ржлрж╛ржЗржи ржХрж░рзЗред  
- ржЙржжрж╛рж╣рж░ржг: V(s) = maxтВР $[R(s,a) + ╬│ ╬г P(s'|s,a) V(s')]$
- ржПржЦрж╛ржирзЗржЗ Dynamic Programming (DP) ржПрж░ ржзрж╛рж░ржгрж╛ ржЖрж╕рзЗ (ржпрзЗржоржи Policy Iteration, Value Iteration)ред  
- рж╕ржорж╕рзНржпрж╛: DP-рждрзЗ ржкрзБрж░рзЛ environment (transition, reward) ржЬрж╛ржирж╛ рж▓рж╛ржЧрзЗред ржХрж┐ржирзНрждрзБ Real-world RL-ржП ржПржЯрж╛ ржЕржЬрж╛ржирж╛!  


### рзй. RL Algorithms:  
ржпрзЗрж╣рзЗрждрзБ Real-world-ржП environment ржЕржЬрж╛ржирж╛, рждрж╛ржЗ Model-Free ржкржжрзНржзрждрж┐ ржжрж░ржХрж╛рж░ред RL Algorithms ржорзВрж▓ржд рждрж┐ржиржЯрж┐ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐рждрзЗ ржмрж┐ржнржХрзНржд:  

#### a. Value-Based (e.g., Q-Learning):  
- рж▓ржХрзНрж╖рзНржп: Optimal value function (Q-value) ржмрзЗрж░ ржХрж░рж╛ред  
- ржЯрзНрж░рзЗржб-ржЕржл: Policy indirectly ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯ (e.g., Q-value ржерзЗржХрзЗ greedy policy)ред  

#### b. Policy-Based (e.g., Policy Gradient):  
- рж▓ржХрзНрж╖рзНржп: рж╕рж░рж╛рж╕рж░рж┐ policy (╧А) ржХрзЗ ржЕржкрзНржЯрж┐ржорж╛ржЗржЬ ржХрж░рж╛ред  
- ржЯрзНрж░рзЗржб-ржЕржл: High variance, ржХрж┐ржирзНрждрзБ complex policy рж╣рзНржпрж╛ржирзНржбрзЗрж▓ ржХрж░рждрзЗ ржкрж╛рж░рзЗред  

#### c. Actor-Critic:  
- рж╣рж╛ржЗржмрзНрж░рж┐ржб: Value-Based (Critic) + Policy-Based (Actor)ред  
- Critic (e.g., TD Error) ржжрж┐рзЯрзЗ Actor-ржХрзЗ ржЧрж╛ржЗржб ржХрж░рж╛ рж╣рзЯред  

---

### рзк. Temporal Difference (TD) Learning:  
Model-Free RL-ржПрж░ рж╣рж╛рж░рзНржЯ рж╣рж▓рзЛ TD Learningред  
- ржЖржЗржбрж┐рзЯрж╛: Monte Carlo (MC) + Dynamic Programming (DP) ржПрж░ ржХржорзНржмрж┐ржирзЗрж╢ржиред  
  - MC: ржкрзБрж░рзЛ ржПржкрж┐рж╕рзЛржб рж╢рзЗрж╖ ржХрж░рзЗ рж╢рзЗржЦрзЗ (high variance)ред  
  - DP: Bootstrapping (ржмрж░рзНрждржорж╛ржи estimate ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ)ред  
- TD: ржкрзНрж░рждрж┐ржЯрж┐ рж╕рзНржЯрзЗржкрзЗ ржЖржкржбрзЗржЯ ржХрж░рзЗ (MC-ржПрж░ ржЪрзЗрзЯрзЗ efficient)ред  
- TD Error: ╬┤ = R + ╬│V(s') - V(s) тЖТ ржПржЗ error ржжрж┐рзЯрзЗ value function ржЖржкржбрзЗржЯ рж╣рзЯред  

---

### рзл. Q-Learning:  
ржПржЯрж┐ Value-Based ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо, TD Learning ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗред  
- рж▓ржХрзНрж╖рзНржп: Optimal Q-value ржЯрзЗржмрж┐рж▓ ржмрж╛ржирж╛ржирзЛ (Q(s,a) = рж╕рзНржЯрзЗржЯ s-ржП ржЕрзНржпрж╛ржХрж╢ржи a-ржПрж░ ржорж╛ржи)ред  
- ржЖржкржбрзЗржЯ рж░рзБрж▓:  
 
  Q(s,a) = Q(s,a) + ╬▒ [R + ╬│ maxтВРтАЩ Q(s',a') - Q(s,a)]
  
 
- ржПржЦрж╛ржирзЗ TD Error (╬┤ = R + ╬│ max Q(s',a') - Q(s,a)) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред  

---

### тЭУ ржХрзЗржи ржПржЗ ржЕрж░рзНржбрж╛рж░рзЗ рж╢рж┐ржЦржЫ?  
рзз. MDP тЖТ RL-ржПрж░ ржЧрж╛ржгрж┐рждрж┐ржХ ржлрж╛ржЙржирзНржбрзЗрж╢ржиред  
рзи. Bellman Equation тЖТ MDP solve ржХрж░рж╛рж░ ржЧрж╛ржгрж┐рждрж┐ржХ ржЯрзБрж▓рж╕ред  
рзй. TD Learning тЖТ Model-Free Environment-ржП Bellman Equation ржЕрзНржпрж╛ржкрзНрж▓рж╛ржЗ ржХрж░рж╛рж░ ржЙржкрж╛рзЯред  
рзк. Q-Learning тЖТ TD + Value-Based ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржорзЗрж░ ржкрзНрж░рж╛ржХржЯрж┐ржХрж╛рж▓ ржЗржоржкрзНрж▓рж┐ржорзЗржирзНржЯрзЗрж╢ржиред  

---

### ЁЯзй ржХрж╛ржирзЗржХрж╢ржиржЧрзБрж▓рзЛ ржПржХ рж▓рж╛ржЗржирзЗ:  
MDP тЖТ Bellman Equation тЖТ Model-Free RL (TD) тЖТ Value-Based (Q-Learning)  

---

### ЁЯУМ ржПржХржЯрж┐ ржЙржжрж╛рж╣рж░ржг ржжрж┐рзЯрзЗ ржмрзБржЭрж┐:  
ржзрж░рзЛ, рждрзБржорж┐ Grid World ржП ржПржХржЯрж┐ agent ржХрзЗ ржЯрзНрж░рзЗржи ржХрж░ржмрзЗред  
- MDP: Grid World-ржХрзЗ рж╕рзНржЯрзЗржЯ, ржЕрзНржпрж╛ржХрж╢ржи, рж░рж┐ржУрзЯрж╛рж░рзНржб ржжрж┐рзЯрзЗ ржбрж┐ржлрж╛ржЗржи ржХрж░рж▓рзЗред  
- Bellman Equation: ржкрзНрж░рждрж┐ржЯрж┐ рж╕рзНржЯрзЗржЯрзЗрж░ value ржХрзНржпрж╛рж▓ржХрзБрж▓рзЗржЯ ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗ (ржпржжрж┐ model ржЬрж╛ржирж╛ ржерж╛ржХрзЗ)ред  
- TD/Q-Learning: Model ржирж╛ ржЬрж╛ржирж╛ ржерж╛ржХрж▓рзЗ, TD-рж░ ржорж╛ржзрзНржпржорзЗ ржЯрзНрж░рж╛рзЯрж╛рж▓ ржПржирзНржб ржПрж░рж░рзЗ рж╢рж┐ржЦржмрзЗред Q-Learning ржжрж┐рзЯрзЗ agent optimal path ржкрж╛ржмрзЗред  

---

### ЁЯФе Confusion ржжрзВрж░ ржХрж░рж╛рж░ Tips:  
рзз. Flowchart ржмрж╛ржирж╛ржУ: MDP тЖТ Bellman тЖТ TD тЖТ Q-Learning тЖТ Policy Gradient тЖТ Actor-Critic.  
рзи. ржкрзНрж░рждрж┐ржЯрж┐ ржЯржкрж┐ржХрзЗрж░ ржЙржжрзНржжрзЗрж╢рзНржп ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░: "ржПржЯрж╛ ржХрзЗржи ржжрж░ржХрж╛рж░?"  
рзй. Compare ржХрж░: TD vs. MC vs. DP, ржмрж╛ Value-Based vs. Policy-Based.  

RL рж╢рзЗржЦрж╛ ржПржХржЯрж┐ рж▓рзЗрзЯрж╛рж░ ржмрж╛ржЗ рж▓рзЗрзЯрж╛рж░ ржкрзНрж░рзЛрж╕рзЗрж╕ред ржкрзНрж░ржержорзЗ ржлрж╛ржЙржирзНржбрзЗрж╢ржи (MDP, Bellman), рждрж╛рж░ржкрж░ ржЯрзБрж▓рж╕ (TD), рждрж╛рж░ржкрж░ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо (Q-Learning)ред ЁЯШК