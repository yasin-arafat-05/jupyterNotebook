<br>
<br>
<br>
<br>

# `#01 Deep Learning VS Reinforcement Learning`


### `**(1) Optimizer vs Optimum Policy:**` 
ЁЯФ╣ **Deep Learning:**  
ржлрж┐ржб-ржлрж░рзЛржпрж╝рж╛рж░рзНржб ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ (FFN) ржмрж╛ ржХржиржнрзЛрж▓рж┐ржЙрж╢ржирж╛рж▓ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУржпрж╝рж╛рж░рзНржХ (CNN)-ржП ржЖржорж╛ржжрзЗрж░ **Optimizer** (SGD, Adam, RMSProp) ржерж╛ржХрзЗ, ржпрж╛ **Loss Function ржорж┐ржирж┐ржорж╛ржЗржЬ ржХрж░рзЗ ржПржмржВ Weight ржЖржкржбрзЗржЯ ржХрж░рзЗред**  

ЁЯФ╣ **Reinforcement Learning:**  
RL-ржП **Optimizer-ржПрж░ рж╕ржорждрзБрж▓рзНржп рж╣рж▓рзЛ Optimum Policyред**   
ржЖржорж░рж╛ ржПржоржи ржПржХржЯрж╛ **Policy $\pi^*$** ржЦрзБржБржЬрж┐, ржпрж╛ ржПржЬрзЗржирзНржЯрзЗрж░ ржЬржирзНржп **рж╕рж░рзНржмрзЛржЪрзНржЪ рж░рж┐ржУржпрж╝рж╛рж░рзНржб ржЖржиржмрзЗред**  

ЁЯТб **рждрж╛рж╣рж▓рзЗ:**  
FFN-ржПрж░ **Optimizer** тЙИ RL-ржПрж░ **Optimum Policy Finding Methods (Q-learning, PPO, A2C, DQN, etc.)**  


### `**(2) Parameter Update ржХрж┐ржнрж╛ржмрзЗ рж╣ржпрж╝? (Gradient Descent vs Bellman Equation):**`  
ЁЯФ╣ **Deep Learning:**  
DL-ржП **Loss Function** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЖржорж░рж╛ **Gradient Descent ржмрж╛ Adam Optimizer** ржжрж┐ржпрж╝рзЗ **Weight ржЖржкржбрзЗржЯ ржХрж░рж┐ред**  

ЁЯФ╣ **Reinforcement Learning:**  
RL-ржП **Loss Function ржирзЗржЗ, ржмрж░ржВ ржЖржорж░рж╛ "Bellman Equation" ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐ред**   
Bellman Equation ржмрзНржпрж╛ржХржУржпрж╝рж╛рж░рзНржб ржкрзНрж░ржкрж╛ржЧрзЗрж╢ржи-ржПрж░ ржорждрзЛ ржХрж╛ржЬ ржХрж░рзЗ, ржпрж╛ ржЖржЧрзЗрж░ Q-value ржЖржкржбрзЗржЯ ржХрж░рзЗ:  


$Q(s, a)$ = $(1 - \alpha) Q(s, a) + \alpha (r + \gamma \max_{a'} Q(s', a'))$

ржПржЯрж╛ Q-value ржХрзЗ ржЖржкржбрзЗржЯ ржХрж░рзЗ, ржпрзЗржЯрж╛ ржПржХ ржзрж░ржирзЗрж░ **"Parameter Update"**ред

ЁЯТб **рждрж╛рж╣рж▓рзЗ:**  
FFN-ржП **Backpropagation** тЙИ RL-ржП **Bellman Equation / Policy Gradient Update**  


### `**(3) Loss Function-ржПрж░ рж╕ржорждрзБрж▓рзНржп ржХрзА RL-ржП?**`  
ЁЯФ╣ **Deep Learning:**  
DL-ржП **Loss Function (MSE, Cross-Entropy)** ржерж╛ржХрзЗ, ржпрж╛ **Prediction vs Ground Truth** рждрзБрж▓ржирж╛ ржХрж░рзЗред  

ЁЯФ╣ **Reinforcement Learning:**  
RL-ржП **Loss Function-ржПрж░ ржмржжрж▓рзЗ Reward Maximization ржХрж╛ржЬ ржХрж░рзЗред**  
ржЖржорж░рж╛ ржПржоржи ржПржХржЯрж╛ **Q-value ржмрж╛ Policy** ржЦрзБржБржЬржЫрж┐, ржпрж╛ **рж╕рж░рзНржмрзЛржЪрзНржЪ ржХрж┐ржЙ-ржнрзНржпрж╛рж▓рзБ ржмрж╛ рж╕рж░рзНржмрзЛржЪрзНржЪ рж░рж┐ржУржпрж╝рж╛рж░рзНржб ржЖржиржмрзЗред**  

тЬЕ **Deep Q-Network (DQN)**: ржПржЦрж╛ржирзЗ **Loss Function ржерж╛ржХрзЗ** тЖТ  

Loss = $(Q_{target} - Q(s, a))^2$
ржПржЯрж╛ MSE-ржПрж░ ржорждрзЛ ржХрж╛ржЬ ржХрж░рзЗред  

тЬЕ **Policy Gradient (PPO, A2C, REINFORCE)**: ржПржЦрж╛ржирзЗ **Reward Maximization Loss Function** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржпрж╝ред  

ЁЯТб **рждрж╛рж╣рж▓рзЗ:**  
FFN-ржПрж░ **Loss Function** тЙИ RL-ржП **Reward Maximization ржмрж╛ Q-value Approximation**  


#### **(4) CNN / RNN-ржПрж░ ржорждрзЛ RL-ржП ржХрзЗржи DQN, PPO, A2C рж╢рж┐ржЦржЫрж┐?**  
ЁЯФ╣ **Deep Learning:**  
DL-ржП ржЖржорж░рж╛ ржмрзБржЭрзЗржЫрж┐,  
- **CNN тЖТ Image Processing ржПрж░ ржЬржирзНржп**  
- **RNN тЖТ Sequential Data ржПрж░ ржЬржирзНржп**  
- **Transformer тЖТ Long-term Dependency Capture ржПрж░ ржЬржирзНржп**  

ЁЯФ╣ **Reinforcement Learning:**  
- **Q-learning тЖТ Discrete Action Space ржПрж░ ржЬржирзНржп**  
- **DQN тЖТ Deep Neural Network ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ Q-learning**  
- **PPO, A2C тЖТ Continuous Action Space ржУ Policy-based Learning ржПрж░ ржЬржирзНржп**  
- **DDPG, SAC тЖТ Continuous Action Control (рж░рзЛржмржЯрж┐ржХрзНрж╕, ржЧрзЗржорж╕)**  

ЁЯТб **рждрж╛рж╣рж▓рзЗ:**  
DL-ржП **CNN тЙИ RL-ржП DQN (Deep Q-Learning)**  
DL-ржП **RNN тЙИ RL-ржП PPO (Policy Optimization for Sequential Actions)**  


<br>

---

---

---

---

<br>
<br>
<br>
<br>
<br>


![image](../img/img02.png)

<br>
<br>
ржПржЗ ржЪрж┐рждрзНрж░ржЯрж┐ **Reinforcement Learning (RL)** ржПрж░ ржорзВрж▓ ржзрж╛рж░ржгрж╛ржХрзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░ржЫрзЗред  

### **ржмрж░рзНржгржирж╛:**  
Reinforcement Learning ржПржХржЯрж┐ рж╢рж┐ржЦржи ржкржжрзНржзрждрж┐ ржпрзЗржЦрж╛ржирзЗ ржПржХржЯрж┐ **Agent (ржПржЬрзЗржирзНржЯ)** ржПржХржЯрж┐ **Environment (ржкрж░рж┐ржмрзЗрж╢)** ржПрж░ рж╕рж╛ржерзЗ ржЗржирзНржЯрж╛рж░ржЕрзНржпрж╛ржХрзНржЯ ржХрж░рзЗ ржПржмржВ **Action (ржХрж░рзНржо)** ржЧрзНрж░рж╣ржг ржХрж░рзЗред ржПрж░ ржорж╛ржзрзНржпржорзЗ ржПржЯрж┐ **Reward (ржкрзБрж░рж╕рзНржХрж╛рж░)** ржкрж╛ржпрж╝ ржПржмржВ **New State (ржирждрзБржи ржЕржмрж╕рзНржерж╛)** рждрзЗ ржкрж░рж┐ржмрж░рзНрждрж┐ржд рж╣ржпрж╝ред    

### **ржкрзНрж░ржзрж╛ржи ржЙржкрж╛ржжрж╛ржирж╕ржорзВрж╣:**  
1я╕ПтГг **Agent (ржПржЬрзЗржирзНржЯ)**:  
   - ржПржЯрж┐ рж╣рж▓ ржПржХржЯрж┐ **ржкрзНрж░рзЛржЧрзНрж░рж╛ржо, ржорзЗрж╢рж┐ржи ржмрж╛ рж░рзЛржмржЯ** ржпрж╛ рж╢рзЗржЦрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзЗред  
   - ржПржЯрж┐ ржкрж░рж┐ржмрзЗрж╢рзЗрж░ рж╕рж╛ржерзЗ ржЗржирзНржЯрж╛рж░ржЕрзНржпрж╛ржХрзНржЯ ржХрж░рзЗ ржПржмржВ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирзЗрзЯред  

2я╕ПтГг **Environment (ржкрж░рж┐ржмрзЗрж╢)**:  
   - ржПржЯрж┐ рж╕рзЗржЗ рж╕рзНржерж╛ржи ржпрзЗржЦрж╛ржирзЗ ржПржЬрзЗржирзНржЯ ржХрж╛ржЬ ржХрж░рзЗред  
   - ржЙржжрж╛рж╣рж░ржг: ржПржХржЯрж┐ ржЧрзЗржо ржмрзЛрж░рзНржб, рж░рж╛рж╕рзНрждрж╛ржпрж╝ рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ ржЧрж╛ржбрж╝рж┐, рж░рзЛржмржЯрзЗрж░ ржЪрж▓рж╛рж░ ржкрже ржЗрждрзНржпрж╛ржжрж┐ред  

3я╕ПтГг **Action (ржХрж░рзНржо)**:  
   - ржПржЬрзЗржирзНржЯ ржХрж┐ржЫрзБ ржХрж╛ржЬ ржХрж░рзЗ, ржпрзЗржоржи **ржЪрж╛рж▓ ржЪрж╛рж▓рж╛ржирзЛ(game), ржПржХржЯрж┐ ржкржжржХрзНрж╖рзЗржк ржирзЗржУржпрж╝рж╛(robot), ржЧрж╛ржбрж╝рж┐рж░ ржЪрж╛ржХрж╛ ржШрзБрж░рж╛ржирзЛ(EV)** ржЗрждрзНржпрж╛ржжрж┐ред  
   - ржПржЬрзЗржирзНржЯ ржмрж┐ржнрж┐ржирзНржи **ржПржХрж╢ржи** ржирж┐рзЯрзЗ рж╢рж┐ржЦрзЗ ржХрзЛржиржЯрж┐ рж╕рзЗрж░рж╛ред  

4я╕ПтГг **Reward (ржкрзБрж░рж╕рзНржХрж╛рж░)**:  
   - ржпржжрж┐ ржПржЬрзЗржирзНржЯ рж╕ржарж┐ржХ ржХрж╛ржЬ ржХрж░рзЗ, ржПржЯрж┐ **ржкржЬрж┐ржЯрж┐ржн рж░рж┐ржУржпрж╝рж╛рж░рзНржб (+)** ржкрж╛ржпрж╝ред  
   - ржпржжрж┐ ржнрзБрж▓ ржХрж╛ржЬ ржХрж░рзЗ, ржПржЯрж┐ **ржирзЗржЧрзЗржЯрж┐ржн рж░рж┐ржУржпрж╝рж╛рж░рзНржб (-)** ржкрж╛ржпрж╝ред  
   - ржЙржжрж╛рж╣рж░ржг:  
     - **ржЧрзЗржо ржЬрзЗрждрж╛ тЖТ + ржкрзБрж░рж╕рзНржХрж╛рж░**  
     - **ржЧрж░рзНрждрзЗ ржкржбрж╝рзЗ ржпрж╛ржУржпрж╝рж╛ тЖТ - рж╢рж╛рж╕рзНрждрж┐**  

5я╕ПтГг **New State (ржирждрзБржи ржЕржмрж╕рзНржерж╛)**:  
   - ржПржЬрзЗржирзНржЯ ржпржЦржи ржХрзЛржирзЛ ржХрж╛ржЬ ржХрж░рзЗ, рждржЦржи ржПржЯрж┐ **ржирждрзБржи ржЕржмрж╕рзНржерж╛ржпрж╝** ржЪрж▓рзЗ ржпрж╛ржпрж╝ред  
   - ржЙржжрж╛рж╣рж░ржг: ржжрж╛ржмрж╛рж░ ржЪрж╛рж▓ ржжрзЗржУрзЯрж╛рж░ ржкрж░ ржирждрзБржи ржЪрзЗрж╕ржмрзЛрж░рзНржбрзЗрж░ ржЕржмрж╕рзНржерж╛ред  

![imgage](../img/img03.png)

<br>
<br>

![image_image](../img/img01.png)


### **Markov Decision Process: Dynamics & Discount (ржмрзНржпрж╛ржЦрзНржпрж╛)**  

ржПржЗ ржЫржмрж┐ржЯрж┐ **Markov Decision Process (MDP)**-ржПрж░ **ржбрж╛рзЯржирж╛ржорж┐ржХрзНрж╕ ржПржмржВ ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржлрзНржпрж╛ржХрзНржЯрж░ (╬│)** ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░ржЫрзЗред  


### **ржзрж╛ржкржЧрзБрж▓рзЛ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг**  

1я╕ПтГг **рж╢рзБрж░рзБ (Initial State):**  
   - ржПржЬрзЗржирзНржЯ (Agent) **sтВА** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ рж╕рзНржЯрзЗржЯ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ, ржпрж╛ **S (State Space)**-ржПрж░ ржЕржВрж╢ред  

2я╕ПтГг **ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рж░рзНржмрж╛ржЪржи (Choosing an Action):**  
   - ржПржЬрзЗржирзНржЯ **aтВА** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗрзЯ, ржпрж╛ **A (Action Space)**-ржПрж░ ржЕржВрж╢ред  

3я╕ПтГг **ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржкрж░рж┐ржмрж░рзНрждржи (State Transition):**  
   - ржПржЬрзЗржирзНржЯ **sтВБ** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржЪрж▓рзЗ ржпрж╛рзЯ, ржпрж╛ **S**-ржПрж░ ржоржзрзНржпрзЗржЗ ржЖржЫрзЗред  
   - ржПржЗ ржкрж░рж┐ржмрж░рзНрждржи ржПржХржЯрж┐ **Transition Probability** ржжрзНржмрж╛рж░рж╛ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рж╣рзЯ:  
     T(sтВА, aтВА, sтВБ)

   - ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУрзЯрж╛рж░ рж╕ржорзЯ ржПржЬрзЗржирзНржЯ ржХрж┐ржЫрзБ **ржкрзБрж░рж╕рзНржХрж╛рж░ (Reward)** ржкрж╛рзЯ:  
     R(sтВА, aтВА, sтВБ)


4я╕ПтГг **ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛ ржЪрж▓рждрзЗ ржерж╛ржХрзЗ (Repeat Until Termination):**  
   - ржПржЗ ржЪржХрзНрж░ (loop) ржЪрж▓рждрзЗ ржерж╛ржХрзЗ ржпрждржХрзНрж╖ржг ржирж╛ ржПржЬрзЗржирзНржЯ ржПржХржЯрж┐ **Terminal State**-ржП ржкрзМржБржЫрзЗ ржмрж╛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╕ржВржЦрзНржпржХ ржзрж╛ржк **T** ржкрзВрж░рзНржг рж╣рзЯред  



### **ржЧрзЛрж▓ (Goal of the Agent)**  
ржПржЬрзЗржирзНржЯ ржПржоржиржнрж╛ржмрзЗ **aтВА, aтВБ, aтВВ, ...** ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржмрзЗ, ржпрж╛рждрзЗ рждрж╛рж░ ржорзЛржЯ ржкрзБрж░рж╕рзНржХрж╛рж░ рж╕рж░рзНржмрзЛржЪрзНржЪ рж╣рзЯ:  

$\sum_{j \geq 0} R(s_j, a_j, s_{j+1})$
ржЕрж░рзНржерж╛рзО, ржПржЬрзЗржирзНржЯ ржнржмрж┐рж╖рзНржпрждрзЗ ржпржд ржмрзЗрж╢рж┐ ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржмрзЗ, рждрж╛рж░ ржЬржирзНржп рждржд ржнрж╛рж▓рзЛ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирж┐рждрзЗ ржЪрж╛рзЯред  



### **рж╕ржорж╕рзНржпрж╛ (Problem of Infinite Sum)**  
- ржПржЦрж╛ржирзЗ ржПржХржЯрж┐ рж╕ржорж╕рзНржпрж╛ рж╣рж▓рзЛ, ржПржЗ **ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржпрзЛржЧржлрж▓ ржЕрж╕рзАржо (infinite sum) рж╣рждрзЗ ржкрж╛рж░рзЗ**ред  
- рждрж╛ржЗ, ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ **ржХржорж┐рзЯрзЗ ржЖржирж╛ (discounted)** рж╣рзЯред  
- ржПржХрзНрж╖рзЗрждрзНрж░рзЗ **Discount Factor (╬│)** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ, ржпрж╛ **0 ржерзЗржХрзЗ 1** ржПрж░ ржоржзрзНржпрзЗ ржерж╛ржХрзЗред  


### **Discounted Total Reward ржлрж░рзНржорзБрж▓рж╛**  

$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

ржПржЦрж╛ржирзЗ,  
тЬЕ **╬│ (Discount Factor)** тЖТ 0 ржПржмржВ 1 ржПрж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ ржорж╛ржи, ржпрж╛ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм ржарж┐ржХ ржХрж░рзЗред  
тЬЕ **╬│ ржмрзЗрж╢рж┐ рж╣рж▓рзЗ (╬│ тЙИ 1)** тЖТ ржПржЬрзЗржирзНржЯ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗржмрзЗред  
тЬЕ **╬│ ржХржо рж╣рж▓рзЗ (╬│ тЙИ 0)** тЖТ ржПржЬрзЗржирзНржЯ рж╢рзБржзрзБржорж╛рждрзНрж░ рждрж╛рзОржХрзНрж╖ржгрж┐ржХ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржЧрзБрж░рзБрждрзНржм ржжрзЗржмрзЗред  


### **ржЙржжрж╛рж╣рж░ржг (Example)**  
ржзрж░рж┐, ржПржХржЬржи рж╕рзНржЯрзБржбрзЗржирзНржЯ ржкрзНрж░рждрж┐ржжрж┐ржи ржкрзЬрж╛рж╢рзЛржирж╛ ржХрж░рж▓рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗ ржнрж╛рж▓рзЛ ржЪрж╛ржХрж░рж┐ ржкрж╛ржмрзЗ (High Reward in Future)**ред  
- ржпржжрж┐ рж╕рзЗ **рж╢рзБржзрзБ ржмрж░рзНрждржорж╛ржи ржЖрж░рж╛ржо ржжрзЗржЦрзЗ (╬│ тЙИ 0)**, рждрж╛рж╣рж▓рзЗ ржкрзЬрж╛рж╢рзЛржирж╛рзЯ ржлрзЛржХрж╛рж╕ ржХрж░ржмрзЗ ржирж╛ред  
- ржпржжрж┐ рж╕рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗрж░ рж▓рж╛ржн ржжрзЗржЦрзЗ (╬│ тЙИ 1)**, рждрж╛рж╣рж▓рзЗ ржжрзАрж░рзНржШржорзЗрзЯрж╛ржжрзА рж▓рж╛ржнрзЗрж░ ржЬржирзНржп ржкрж░рж┐рж╢рзНрж░ржо ржХрж░ржмрзЗред  

### **рж╕ржВржХрзНрж╖рзЗржкрзЗ:**  
тЬЕ **Markov Decision Process (MDP)** ржПржЬрзЗржирзНржЯрзЗрж░ **ржЕрзНржпрж╛ржХрж╢ржи ржПржмржВ рж░рж┐ржУрзЯрж╛рж░рзНржб** ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред  
тЬЕ **Transition Probability** ржмрж▓рзЗ ржжрзЗрзЯ, ржПржХ рж╕рзНржЯрзЗржЯ ржерзЗржХрзЗ ржЕржирзНржп рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУрзЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ред  
тЬЕ **Discount Factor (╬│)** ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред  
тЬЕ **╬│ тЙИ 1 рж╣рж▓рзЗ** ржПржЬрзЗржирзНржЯ ржжрзАрж░рзНржШржорзЗрзЯрж╛ржжрзА рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирзЗрзЯ, **╬│ тЙИ 0 рж╣рж▓рзЗ** рждрж╛рзОржХрзНрж╖ржгрж┐ржХ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯред  

### **╬│ (ржЧрж╛ржорж╛) ржПржмржВ j ржХрзА?**  

#### **╬│ (ржЧрж╛ржорж╛) ржХрзА?**  
ржЧрж╛ржорж╛ $\gamma$ рж╣рж▓ **discount factor** (ржЫрж╛ржбрж╝рзЗрж░ рж╣рж╛рж░), ржпрж╛ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржмрж░рзНрждржорж╛ржирзЗрж░ рждрзБрж▓ржирж╛ржпрж╝ ржХрждржЯрж╛ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣ржмрзЗред  

- **$\gamma$ ржПрж░ ржорж╛ржи 0 ржерзЗржХрзЗ 1 ржПрж░ ржоржзрзНржпрзЗ ржерж╛ржХрзЗ $0 \leq \gamma \leq 1$**  
- ржпржжрж┐ **$\gamma$ ржмрзЗрж╢рж┐ рж╣ржпрж╝ $\gamma \approx 1$**, рждрж╛рж╣рж▓рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржмрж░рзНрждржорж╛ржирзЗрж░ ржорждрзЛржЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж░рж╛ рж╣ржпрж╝ред  
- ржпржжрж┐ **$\gamma$ ржХржо рж╣ржпрж╝ $\gamma \approx 0$**, рждрж╛рж╣рж▓рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж░рж╛ рж╣ржпрж╝ ржПржмржВ рждрж╛ ржжрзНрж░рзБржд рж╣рж╛рж░рж┐ржпрж╝рзЗ ржпрж╛ржпрж╝ред  


#### **j ржХрзА?**  
**j рж╣рж▓рзЛ ржЯрж╛ржЗржо рж╕рзНржЯрзЗржк (Time Step) ржмрж╛ ржкрж░рзНржпрж╛ржпрж╝**ред ржПржЯрж┐ ржирж┐рж░рзНржжрзЗрж╢ ржХрж░рзЗ **ржХржд ржиржорзНржмрж░ рж╕ржоржпрж╝рзЗ ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУржпрж╝рж╛ рж╣ржЪрзНржЫрзЗ**ред  

- j = 0 тЖТ ржкрзНрж░ржержо рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ (immediate reward) тЖТ **$\gamma^0 = 1$** (ржХрзЛржи ржЫрж╛ржбрж╝ ржирзЗржЗ)  
- j = 1  тЖТ ржкрж░рзЗрж░ рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^1$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- j = 2  тЖТ рждрж╛рж░ ржкрж░рзЗрж░ рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^2$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- j = 3  тЖТ ржЖрж░ржУ ржкрж░рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^3$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- ржПржЗржнрж╛ржмрзЗ **ржкрзНрж░рждрзНржпрзЗржХ ржзрж╛ржкрзЗ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржорзВрж▓рзНржп ржХржорждрзЗ ржерж╛ржХрзЗ**  


#### **ржХрзЗржи ╬│^j ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржпрж╝?**  
ржЖржорж░рж╛ ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржХрж░рж╛ ржЯрзЛржЯрж╛рж▓ рж░рж┐ржУржпрж╝рж╛рж░рзНржб (Discounted Total Reward) рж╣рж┐рж╕рж╛ржм ржХрж░рж┐:  


$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

ржПржЦрж╛ржирзЗ:  
тЬЕ **$\gamma^j$ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржЫрзЛржЯ ржХрж░рзЗ ржжрзЗржпрж╝**, ржпрж╛рждрзЗ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЕржмржжрж╛ржи ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржХржорждрзЗ ржерж╛ржХрзЗред  
тЬЕ **j ржпржд ржмржбрж╝ рж╣ржпрж╝, $\gamma^j$ рждржд ржЫрзЛржЯ рж╣ржпрж╝**, ржЕрж░рзНржерж╛рзО ржЕржирзЗржХ ржжрзВрж░рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣ржпрж╝рзЗ ржпрж╛ржпрж╝ред  
тЬЕ **ржПржЯрж┐ ржЗржиржлрж┐ржирж┐ржЯ (ржЕрж╕рзАржо) рж░рж┐ржУржпрж╝рж╛рж░рзНржб ржЕрзНржпрж╛ржХрзБржорзБрж▓рзЗрж╢ржи рж░рзЛржз ржХрж░рзЗ**, ржпрж╛рждрзЗ рж░рж┐ржУржпрж╝рж╛рж░рзНржбрзЗрж░ рж╣рж┐рж╕рж╛ржм рж╕рзАржорж┐ржд ржерж╛ржХрзЗред  

---

#### **ржЙржжрж╛рж╣рж░ржг**  
ржзрж░рзБржи, ржкрзНрж░рждрж┐ржЯрж┐ ржзрж╛ржкрзЗ ржПржЬрзЗржирзНржЯ **рззрзж ржкржпрж╝рзЗржирзНржЯ** ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржЪрзНржЫрзЗ, ржПржмржВ **ржЧрж╛ржорж╛ $\gamma$ = рзж.рзп**ред рждрж╛рж╣рж▓рзЗ:  

$R_{total} = 10 \times (0.9^0) + 10 \times (0.9^1) + 10 \times (0.9^2) + 10 \times (0.9^3)$ + ...

= 10 + 9 + 8.1 + 7.29 + 6.56 + ...


ржПржЦрж╛ржирзЗ ржжрзЗржЦрж╛ ржпрж╛ржЪрзНржЫрзЗ ржпрзЗ, **ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржкрж░рж┐ржорж╛ржг ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржХржоржЫрзЗ ржХрж╛рж░ржг $\gamma^j$ ржЫрзЛржЯ рж╣ржпрж╝рзЗ ржпрж╛ржЪрзНржЫрзЗ**ред

<br>

---

<br>

---

<br>

### **ржкрж▓рж┐рж╕рж┐ $\pi$ ржХрзАржнрж╛ржмрзЗ ржПрж▓рзЛ ржПржмржВ ржХрзЗржи ржжрж░ржХрж╛рж░?**  

#### **ЁЯУМ рзз. ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ ржХрзА ржЫрж┐рж▓ ржПржмржВ ржПрждрзЗ ржХрзА ржмрзЛржЭрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ?**  
ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржЯрж┐ ржЫрж┐рж▓тАФ  


$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

- **ржПржЗ ржлрж░рзНржорзБрж▓рж╛ ржмрж▓рзЗ, ржпржжрж┐ ржЖржорж░рж╛ $s_0$ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рж┐, рждрж╛рж╣рж▓рзЗ ржорзЛржЯ ржХржд ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржЖрж╢рж╛ ржХрж░рж╛ ржпрж╛ржпрж╝ред**  

ржХрж┐ржирзНрждрзБ рж╕ржорж╕рзНржпрж╛ рж╣рж▓рзЛ, **ржПржЬрзЗржирзНржЯ ржХрзАржнрж╛ржмрзЗ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржмрзЗ, рж╕рзЗржЯрж┐ ржПржЦрж╛ржирзЗ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣ржпрж╝ржирж┐!**  


#### **ЁЯУМ рзи. рждрж╛рж╣рж▓рзЗ ржкрж▓рж┐рж╕рж┐рж░ $\pi$ ржжрж░ржХрж╛рж░ ржХрзЗржи?**  
ржЖржорж░рж╛ ржЬрж╛ржирж┐, **reinforcement learning-ржПрж░ ржорзВрж▓ ржХрж╛ржЬ рж╣рж▓рзЛ рж╕ржарж┐ржХ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржУрзЯрж╛, ржпрж╛рждрзЗ рж╕рж░рзНржмрзЛржЪрзНржЪ ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ред** ржХрж┐ржирзНрждрзБ ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржпрж╝ action $a_j$ ржХрзАржнрж╛ржмрзЗ ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗ, рждрж╛ ржмрж▓рж╛ рж╣рзЯржирж┐ред  

ржПржЬрзЗржирзНржЯржХрзЗ ржпржжрж┐ **ржПржХржЯрж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржирж┐ржпрж╝ржо ржЕржирзБрж╕рж╛рж░рзЗ ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рждрзЗ ржмрж▓рж╛ рж╣ржпрж╝**, рждрж╛рж╣рж▓рзЗ рж╕рзЗржЯрж╛ржЗ **ржкрж▓рж┐рж╕рж┐ (Policy, $\pi$**!  


$\pi : S \rightarrow A$

ржорж╛ржирзЗ, **ржкрж▓рж┐рж╕рж┐ ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи, ржпрж╛ ржмрж▓рзЗ ржжрзЗржпрж╝ ржкрзНрж░рждрж┐ржЯрж┐ state-ржП ржХрзЛржи action ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗред**  

ржпрзЗржоржиржГ  
- ржпржжрж┐ **s = ржХрзНрж╖рзБржзрж╛рж░рзНржд (hungry)** рж╣рзЯ, рждрж╛рж╣рж▓рзЗ **$\pi(s)$ = ржЦрж╛ржУ (eat)**ред  
- ржпржжрж┐ **s = ржЯрзНрж░рж╛ржлрж┐ржХ рж▓рж╛ржЗржЯ рж▓рж╛рж▓ (red light)** рж╣рзЯ, рждрж╛рж╣рж▓рзЗ **$\pi(s)$ = ржжрж╛ржБрзЬрж╛ржУ (stop)**ред  

ЁЯУМ **рж╕рзЛржЬрж╛ ржнрж╛рж╖рж╛ржпрж╝, ржкрж▓рж┐рж╕рж┐ рж╣рж▓рзЛ ржПржоржи ржПржХржЯрж┐ ржирж┐ржпрж╝ржо, ржпрж╛ state ржЕржирзБржпрж╛ржпрж╝рзА action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред**  


#### **ЁЯУМ рзй. ржкрж▓рж┐рж╕рж┐рж░ ржЬржирзНржп ржирждрзБржи ржлрж░рзНржорзБрж▓рж╛ ржХрзА?**  

ржпрзЗрж╣рзЗрждрзБ ржкрж▓рж┐рж╕рж┐ **ржкрзНрж░рждрж┐ржЯрж┐ state-ржПрж░ ржЬржирзНржп action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ**, рждрж╛ржЗ ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржХрзЗ ржЖржкржбрзЗржЯ ржХрж░рзЗ **policy-based equation** ржмрж╛ржирж╛ржирзЛ рж╣ржпрж╝тАФ  


$V_{\pi}(s) = E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) | s_0 = s \right]$

ржПржЦрж╛ржирзЗ,  
- **$\pi(s_j)$** тЖТ ржПржЦржи ржЖржорж░рж╛ ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ action $a_j$ рж▓рж┐ржЦржЫрж┐ ржирж╛, ржХрж╛рж░ржг **ржкрж▓рж┐рж╕рж┐ ржЖржЧрзЗ ржерзЗржХрзЗржЗ ржмрж▓рзЗ ржжрж┐ржЪрзНржЫрзЗ, ржкрзНрж░рждрж┐ржЯрж┐ state-ржП ржХрзЛржи action ржирж┐рждрзЗ рж╣ржмрзЗ!**  
- **$V_{\pi}(s)$** тЖТ ржПрж░ ржорж╛ржирзЗ, **ржПржЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ policy $\pi$ ржЕржирзБрж╕рж░ржг ржХрж░рж▓рзЗ, state (s) ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ ржорзЛржЯ ржХржд ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржмрзЗ**ред  

ЁЯУМ **ржорзВрж▓ ржкрж╛рж░рзНржержХрзНржп:**  
- ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржпрж╝ action ржЫрж┐рж▓ **$a_j$**, ржпрзЗржЯрж┐ ржЖржорж░рж╛ ржирж┐ржЬрзЗрж░рж╛ ржарж┐ржХ ржХрж░рж┐ржирж┐ред  
- ржПржЦржи **$a_j = \pi(s_j)$**, ржорж╛ржирзЗ **policy рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░ржЫрзЗред**  
 
<br>

# `## Conditional Expectation (E) vs Conditional Probability (P):`

`In the formula:`

P(A | B) = $\text{B ржШржЯрзЗржЫрзЗ, рждржЦржи A ржШржЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛}$
ржПржЦржи, ржЖржорж╛ржжрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржЯрж┐:  

$E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$

ржПржЦрж╛ржирзЗ, **" $\mid s_0 = s$ "** ржЕржВрж╢ржЯрж╛ ржЖрж╕рж▓рзЗ рж╢рж░рзНржд (condition) ржмрзЛржЭрж╛ржЪрзНржЫрзЗред рждрж╛ржЗ ржЖржорж░рж╛ ржмрж▓рждрзЗ ржкрж╛рж░рж┐,  

ЁЯУМ **ржЖржорж░рж╛ ржПржоржи ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи (Expected Value) ржмрзЗрж░ ржХрж░ржЫрж┐ ржпрзЗржЦрж╛ржирзЗ $s_0 = s $ ржПржЗ рж╢рж░рзНрждржЯрж┐ ржорзЗржирзЗ ржирзЗржУржпрж╝рж╛ рж╣ржпрж╝рзЗржЫрзЗред**  


# **ЁЯУМ рждрж╛рж╣рж▓рзЗ, ржПржЦрж╛ржирзЗ рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛рж░ (Conditional Probability) рж╕ржорзНржкрж░рзНржХ ржХрзА?**  

## **тЬЕ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛ (Conditional Expectation) ржмржирж╛ржо рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛ (Conditional Probability)**
 
Reinforcement Learning (RL)-ржП ржЖржорж░рж╛ ржорзВрж▓ржд **рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐, ржХрж┐ржирзНрждрзБ ржПрж░ ржкрж┐ржЫржирзЗ **рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛рж░** ржзрж╛рж░ржгрж╛ рж░рзЯрзЗржЫрзЗред  

### **ЁЯФ╣ рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛ (Conditional Probability)**

P(A | B) = $\frac{P(A \cap B)}{P(B)}$
ЁЯФ╣ ржПржЯрж┐ ржмрзЛржЭрж╛рзЯ ржпрзЗ, **B ржШржЯржирж╛ ржШржЯрзЗ ржЧрзЗржЫрзЗ, рждржЦржи A ржШржЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ ржХржд?**  
ЁЯФ╣ Reinforcement Learning-ржП **ржкрж░ржмрж░рзНрждрзА рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛** ржмрзЛржЭрж╛рждрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред  
ЁЯФ╣ ржЙржжрж╛рж╣рж░ржг:  
   - рждрзБржорж┐ ржпржжрж┐ ржПржЦржи **$s_t$** рж╕рзНржЯрзЗржЯрзЗ ржерж╛ржХрзЛ ржПржмржВ **$a_t$** ржЕрзНржпрж╛ржХрж╢ржи ржирж╛ржУ, рждрж╛рж╣рж▓рзЗ **ржкрж░ржмрж░рзНрждрзА рж╕рзНржЯрзЗржЯрзЗ $s_{t+1}$ ржпрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ ржХржд?**  
   - ржЕрж░рзНржерж╛рзО, $P(s_{t+1} | s_t, a_t)$


### **ЁЯФ╣ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛ (Conditional Expectation)**

$E[X | Y] = \sum_{x} x P(X = x | Y)$
ЁЯФ╣ ржПржЯрж┐ ржмрж▓рзЗ ржпрзЗ, **Y ржШржЯрзЗ ржпрж╛ржУржпрж╝рж╛рж░ ржкрж░ X ржПрж░ ржЧрзЬ ржорж╛ржи (expected value) ржХржд?**  
ЁЯФ╣ Reinforcement Learning-ржП **ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржи** ржирж┐рж░рзНржзрж╛рж░ржгрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред  
ЁЯФ╣ ржЙржжрж╛рж╣рж░ржг:  
   - ржпржжрж┐ $s_0$ = s  ржШржЯрзЗ ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗ ржкрзНрж░рж╛ржкрзНржд рж╕ржорж╕рзНржд ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзЬ ржорж╛ржи** ржХрзА рж╣ржмрзЗ?  
   - ржЕрж░рзНржерж╛рзО,  
     
   $V_\pi(s)$ = $E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$

   - ржПржЦрж╛ржирзЗ **$s_0 = s$** ржзрж░рзЗ ржирж┐рзЯрзЗ ржнржмрж┐рж╖рзНржпрзО ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи ржмрзЗрж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗред  

## **тЬЕ Reinforcement Learning-ржП Policy ржирж┐рж░рзНржзрж╛рж░ржгрзЗрж░ рж╕рзВрждрзНрж░**
### **ЁЯУМ Policy $\pi$ ржорж╛ржирзЗ ржХрзА?**  
ЁЯФ╣ Policy рж╣рж▓рзЛ ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи ржпрж╛ ржмрж▓рзЗ **"ржкрзНрж░рждрзНржпрзЗржХ рж╕рзНржЯрзЗржЯрзЗ ржХрзЛржи ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржУржпрж╝рж╛ ржЙржЪрж┐ржд?"**  
ЁЯФ╣ ржЕрж░рзНржерж╛рзО,  

   $\pi : S \to A$
   **ржпрзЗржЦрж╛ржирзЗ,**  
   - S = рж╕рзНржЯрзЗржЯ рж╕рзНржкрзЗрж╕  
   - A = ржЕрзНржпрж╛ржХрж╢ржи рж╕рзНржкрзЗрж╕  


### **ЁЯУМ Policy ржирж┐рж░рзНржзрж╛рж░ржгрзЗрж░ ржлрж░рзНржорзБрж▓рж╛**

$V_\pi(s) = E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$

ЁЯФ╣ **ржПржЦрж╛ржирзЗ:**  
   -  $V_\pi(s)$ = **рж╕рзНржЯрзЗржЯ s-ржП ржерж╛ржХрж╛рж░ ржорж╛ржи (value function)**  
   - **$E[\cdot]$** = **ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи (Expectation)**  
   - **$\gamma^j$** = **ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржлрзНржпрж╛ржХрзНржЯрж░** (ржнржмрж┐рж╖рзНржпрждрзЗрж░ рж░рж┐ржУржпрж╝рж╛рж░рзНржбрзЗрж░ ржЧрзБрж░рзБрждрзНржм ржХржорж╛ржирзЛрж░ ржЬржирзНржп)  
   - **$R(s_j, \pi(s_j), s_{j+1})$** = **ржмрж░рзНрждржорж╛ржи рж░рж┐ржУржпрж╝рж╛рж░рзНржб**  
   - **$\pi(s_j)$** = **ржкрж▓рж┐рж╕рж┐ ржЕржирзБржпрж╛рзЯрзА ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рж░рзНржмрж╛ржЪржи**  

тЬЕ **ржПржЯрж┐ ржЖрж╕рж▓рзЗ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛** (Conditional Expectation), ржХрж╛рж░ржг ржЖржорж░рж╛ $s_0$ = s ржзрж░рзЗ ржирж┐рзЯрзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзЬ ржорж╛ржи рж╣рж┐рж╕рж╛ржм ржХрж░ржЫрж┐ред  


#### **ЁЯУМ рзк. Reinforcement Learning-ржП Policy ржХрзЗржи ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг?**  
тЬЕ **Policy $\pi$ ржЫрж╛рзЬрж╛ ржПржЬрзЗржирзНржЯ ржЬрж╛ржиржмрзЗ ржирж╛, ржХрзЛржи action ржирж┐рж▓рзЗ рж╕рж░рзНржмрзЛржЪрзНржЪ reward ржкрж╛ржУрзЯрж╛ ржпрж╛ржмрзЗред**  
тЬЕ **ржЖржорж╛ржжрзЗрж░ ржорзВрж▓ рж▓ржХрзНрж╖рзНржп рж╣рж▓рзЛ optimal policy ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛, ржпрж╛ рж╕рж░рзНржмрзЛржЪрзНржЪ ржкрзБрж░рж╕рзНржХрж╛рж░ ржжрзЗржмрзЗред**  
тЬЕ **Policy ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж▓рзЗ, ржПржЬрзЗржирзНржЯ рж╕рж╣ржЬрзЗржЗ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирж┐рждрзЗ ржкрж╛рж░рзЗ ржПржмржВ environment ржерзЗржХрзЗ рж╢рзЗржЦрж╛рж░ ржХрзНрж╖ржорждрж╛ ржкрж╛ржпрж╝ред**  

<br> 

### Important: 
Optimal Policy(ржПржХржЯрж╛ ржХрж╛ржЬ рждрзЛ ржЕржирзЗржХ ржнрж╛ржмрзЗ ржХрж░рж╛ ржпрж╛ржмрзЗ, рждрж╛рж░ ржоржзрзНржпрзЗ рж╕ржмржЪрзЗрзЯрзЗ рж╕рж╣ржЬ ржпрзЗржЗржнрж╛ржмрзЗ ржХрж░рж╛ ржпрж╛ржмрзЗ рж╕рзЗржЗржЯрж╛ рж╣ржЪрзНржЫрзЗ optimal policy)

<br>

![imgage](../img/img04.png)

<br>


In our car example, our optimal policy will be like below: 

<br> 

![image](../img/img06.png)

<br> 

# **рж╕рзНржЯрзЗржЯ-ржЕрзНржпрж╛ржХрж╢ржи ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржи (State-Action Value Function) ржХрзА?**  

**рж░рж┐ржЗржиржлрзЛрж░рзНрж╕ржорзЗржирзНржЯ рж▓рж╛рж░рзНржирж┐ржВ (RL) ржПржмржВ ржорж╛рж░рзНржХржн ржбрж┐рж╕рж┐рж╢ржи ржкрзНрж░рж╕рзЗрж╕ (MDP)-ржП рж╕рзНржЯрзЗржЯ-ржЕрзНржпрж╛ржХрж╢ржи ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржи** рж╣рж▓рзЛ ржПржоржи ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи ржпрж╛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ **рж╕рзНржЯрзЗржЯ (state) ржУ ржЕрзНржпрж╛ржХрж╢ржи (action)** ржерзЗржХрзЗ ржнржмрж┐рж╖рзНржпрждрзЗ ржХрзА ржкрж░рж┐ржорж╛ржг ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУрзЯрж╛ ржпрзЗрждрзЗ ржкрж╛рж░рзЗ рждрж╛ ржЕржирзБржорж╛ржи ржХрж░рзЗред  


### **ЁЯУМ рж╕ржВржЬрзНржЮрж╛:**  
рж╕рзНржЯрзЗржЯ-ржЕрзНржпрж╛ржХрж╢ржи ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржиржХрзЗ $Q(s, a)$  ржжрзНржмрж╛рж░рж╛ ржкрзНрж░ржХрж╛рж╢ ржХрж░рж╛ рж╣рзЯред ржПржЯрж┐ ржмрзЛржЭрж╛рзЯ, ржпржжрж┐ **$ s $** рж╕рзНржЯрзЗржЯрзЗ ржерзЗржХрзЗ **$ a $** ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржУрзЯрж╛ рж╣рзЯ, рждрж╛рж╣рж▓рзЗ ржнржмрж┐рж╖рзНржпрждрзЗ ржХрзА ржкрж░рж┐ржорж╛ржг **expected reward** ржкрж╛ржУрзЯрж╛ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред  

Q(s, a) = $\mathbb{E} [ R_t | S_t = s, A_t = a]$

ржПржЦрж╛ржирзЗ,  
- s  = ржмрж░рзНрждржорж╛ржи рж╕рзНржЯрзЗржЯ  
- a  = ржирзЗржУрзЯрж╛ ржЕрзНржпрж╛ржХрж╢ржи  
- $R_t$ = ржнржмрж┐рж╖рзНржпрждрзЗ рж╕ржорзНржнрж╛ржмрзНржп рж░рж┐ржУрзЯрж╛рж░рзНржбрзЗрж░ ржпрзЛржЧржлрж▓  
- $\mathbb{E}$  = ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи (Expected Value)  

<br>

![image](../img/img07.png) 

<br>

### **ЁЯУМ 1я╕ПтГг State-Action Value Function $Q^*(s, a)$  ржХрзА?**  
- **$Q^*(s, a)$** рж╣рж▓рзЛ **state s -ржП ржерзЗржХрзЗ action a  ржирзЗржУржпрж╝рж╛рж░ ржкрж░ рж╕рж░рзНржмрзЛржЪрзНржЪ рж╕ржорзНржнрж╛ржмрзНржп reward ржПрж░ ржорж╛ржи**ред  
- ржПржЯрж┐ **MDP (Markov Decision Process)** ржПрж░ ржоржзрзНржпрзЗ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рж╣рзЯред  
- рж╕ржВржЬрзНржЮрж╛ ржЕржирзБрж╕рж╛рж░рзЗ, $Q^*(s, a)$ ржирж┐ржорзНржирж▓рж┐ржЦрж┐ржд рж╕ржорзАржХрж░ржг ржжрзНржмрж╛рж░рж╛ ржкрзНрж░ржХрж╛рж╢ ржХрж░рж╛ рж╣рзЯ:


- Equation: 

$Q^*(s, a)$ = 

$\sum_{s' \in S}$ $T(s, a, s')$ $R(s, a, s')$ + $\gamma \sum_{s' \in S}$ $T(s, a, s')$ $V_{\pi^*}(s')$

ржПржЦрж╛ржирзЗ,  
  - $T(s, a, s')$  тЖТ s state ржерзЗржХрзЗ  a  action ржирзЗржУржпрж╝рж╛рж░ ржкрж░  s' state-ржП ржпрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ред  
  - **$R(s, a, s')$** тЖТ state s -ржП action a ржирж┐рж▓рзЗ  s' -ржП ржпрж╛ржУржпрж╝рж╛рж░ ржЬржирзНржп ржкрзНрж░рж╛ржкрзНржд rewardред  
  - **$\gamma$** тЖТ **discount factor**, ржпрж╛ ржнржмрж┐рж╖рзНржпрждрзЗрж░ reward-ржПрж░ ржЧрзБрж░рзБрждрзНржм ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред  
  - **$V_{\pi^*}(s')$** тЖТ **рж╕рзЗрж░рж╛ policy ржЕржирзБрж╕рж╛рж░рзЗ ржкрж░ржмрж░рзНрждрзА state s'-ржПрж░ ржЬржирзНржп value functionред**  

### **ЁЯУМ 3я╕ПтГг ржПржЗ ржлрж╛ржВрж╢ржи ржХрзА ржмрзЛржЭрж╛ржпрж╝?**  
1я╕ПтГг ржПржЯрж┐ ржмрж▓рзЗ **ржХрзЛржирзЛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ state s ржерзЗржХрзЗ ржПржХржЯрж┐ action  a ржирзЗржУржпрж╝рж╛рж░ ржкрж░ рж╕рж░рзНржмрзЛржЪрзНржЪ reward ржХрзА рж╣рждрзЗ ржкрж╛рж░рзЗ**ред  
2я╕ПтГг ржПржЯрж┐ **Bellman Equation** ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ рждрзИрж░рж┐ ржПржмржВ Reinforcement Learning-ржП **Q-learning** ржПржмржВ **Deep Q-Networks (DQN)** ржПрж░ ржорждрзЛ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржорзЗрж░ ржнрж┐рждрзНрждрж┐ред  
3я╕ПтГг **Optimal Q-function $Q^*(s, a)$** ржЖржорж╛ржжрзЗрж░ ржЬрж╛ржирж╛ржпрж╝ **рж╕рзЗрж░рж╛ рж╕ржорзНржнрж╛ржмрзНржп рж╕рж┐ржжрзНржзрж╛ржирзНржд ржХрзА рж╣ржУржпрж╝рж╛ ржЙржЪрж┐ржд**ред  

### **ЁЯФН `argmax` ржорж╛ржирзЗ ржХрзА ржПржЦрж╛ржирзЗ?**  

тЬЕ ржПржЦрж╛ржирзЗ **$\arg\max$** ржорж╛ржирзЗ рж╣рж▓рзЛ **"ржХрзЛржирзЛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ state \( s \)-ржП, ржПржоржи action \( a \) ржмрзЗрж░ ржХрж░рж╛ ржпрж╛ state-action value function $Q^*(s,a)$ рж╕рж░рзНржмрзЛржЪрзНржЪ ржХрж░рзЗред"**  

### **ЁЯУМ рж╕рж╣ржЬржнрж╛ржмрзЗ ржмрзБржЭрж┐:**  
ЁЯФ╣ **`max`** ржмрж▓рзЗ ржжрзЗрзЯ **рж╕рж░рзНржмрзЛржЪрзНржЪ ржорж╛ржи ржХржд**ред  
ЁЯФ╣ **`argmax`** ржмрж▓рзЗ ржжрзЗрзЯ **рж╕рж░рзНржмрзЛржЪрзНржЪ ржорж╛ржи ржХрзЛржерж╛рзЯ ржЖржЫрзЗ ржмрж╛ ржХрзЛржи `a` (action) ржирзЗржУрзЯрж╛ ржЙржЪрж┐ржд**ред  

ЁЯСЙ **рждрж╛рж╣рж▓рзЗ**  
$\pi^*(s) = \arg\max Q^*(s, a)$  ржорж╛ржирзЗ рж╣рж▓рзЛ:  
тЮбя╕П **ржХрзЛржи action \( a \) ржирзЗржУрзЯрж╛ рж╣рж▓рзЗ Q-value рж╕ржмржЪрзЗрзЯрзЗ ржмрзЗрж╢рж┐ рж╣ржмрзЗ, рж╕рзЗржЯрж╛ржЗ optimal policy \( \pi^*(s) \)** ржмрзЗржЫрзЗ ржирзЗржмрзЗред  


### **ЁЯОп рж╕рж╣ржЬ ржЙржжрж╛рж╣рж░ржг:**  
ржзрж░рзЛ, рждрзБржорж┐ **ржПржХржЯрж╛ рж░рзЛржмржЯ** ржмрж╛ржирж┐рзЯрзЗржЫрзЛ, ржпрж╛ **ржмрж┐ржнрж┐ржирзНржи action ржирж┐рзЯрзЗ reward ржкрж╛рзЯ**ред  

| Action (a) | Q-value $Q^*(s,a)$ |  
|--------------|----------------|  
| Move Left   | **3** |  
| Move Right  | **5** |  
| Jump        | **2** |  

ржПржЦржи,  
ЁЯФ╣ $max(Q^*(s,a))$ 
 = 5 (рж╕рж░рзНржмрзЛржЪрзНржЪ Q-value)  
ЁЯФ╣ $argmax(Q^*(s,a))$ 
 = Move Right (ржпрзЗ action ржирж┐рж▓рзЗ ржПржЗ value ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯ)  

ЁЯУМ рждрж╛рж╣рж▓рзЗ **optimal policy $\pi^*(s)$ = Move Right**, ржХрж╛рж░ржг ржПржЗ action ржирж┐рж▓рзЗ рж░рзЛржмржЯ рж╕ржмржЪрзЗрзЯрзЗ ржнрж╛рж▓рзЛ reward ржкрж╛ржмрзЗред  


### тЬЕ рж╕ржВржХрзНрж╖рзЗржкрзЗ ржоржирзЗ рж░рж╛ржЦрж╛рж░ ржЙржкрж╛рзЯ:**  
1я╕ПтГг `max` тЖТ рж╕рж░рзНржмрзЛржЪрзНржЪ ржорж╛ржи **ржХржд?**  
2я╕ПтГг `argmax` тЖТ **рж╕рж░рзНржмрзЛржЪрзНржЪ ржорж╛ржи ржХрзЛржерж╛рзЯ ржмрж╛ ржХрзЛржи action ржирзЗржУрзЯрж╛ ржЙржЪрж┐ржд?

<br>
<br>

# `# Q-learning: `

<br>

Q-learning рж╣рж▓ ржПржХржЯрж┐ ржЬржиржкрзНрж░рж┐ржпрж╝ рж░рж┐ржЗржиржлрзЛрж░рзНрж╕ржорзЗржирзНржЯ рж▓рж╛рж░рзНржирж┐ржВ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо ржпрж╛ ржПржХржЯрж┐ ржЕржкржЯрж┐ржорж╛рж▓ рж╕рзНржЯрзЗржЯ-ржЕрзНржпрж╛ржХрж╢ржи ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржи $Q(s, a)$ рж╢рзЗржЦрж╛рж░ ржЪрзЗрж╖рзНржЯрж╛ ржХрж░рзЗред ржЙржкрж░рзЗрж░ ржЫржмрж┐рж░ рждржерзНржп ржЕржирзБржпрж╛ржпрж╝рзА, Q-learning ржирж┐ржорзНржирж▓рж┐ржЦрж┐ржд ржзрж╛ржкрзЗ ржХрж╛ржЬ ржХрж░рзЗ:

<br>

![image](../img/img08.png)

<br>

### **Q-learning ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо**:
1. **рж╢рзБрж░рзБрждрзЗ** $Q(s, a)$ ржПрж░ ржЬржирзНржп рж░тАНрзНржпрж╛ржирзНржбржо ржнрзНржпрж╛рж▓рзБ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣ржпрж╝ (ржпрзЗржоржи, рж╕ржм рж╢рзВржирзНржп)ред
2. **ржПржХржЯрж┐ ржкрзНрж░рж╛ржержорж┐ржХ ржЕржмрж╕рзНржерж╛ $s_0$ ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рж╛ рж╣ржпрж╝**ред
3. **рж╕рж░рзНржмрзЛрждрзНрждржо ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рж░рзНржмрж╛ржЪржи ржХрж░рж╛ рж╣ржпрж╝**:  
   $a_0$ = $\arg\max_{a \in A} Q(s_0, a)$
   ржЕрж░рзНржерж╛рзО, ржмрж░рзНрждржорж╛ржи ржЕржмрж╕рзНржерж╛рж░ ржЬржирзНржп ржпрзЗржЗ ржЕрзНржпрж╛ржХрж╢ржирзЗрж░ $Q(s, a)$ ржорж╛ржи рж╕рж░рзНржмрзЛржЪрзНржЪ, рж╕рзЗржЯрж┐ ржмрзЗржЫрзЗ ржирзЗржУржпрж╝рж╛ рж╣ржпрж╝ред
4. **ржирж┐рж░рзНржмрж╛ржЪрж┐ржд ржЕрзНржпрж╛ржХрж╢ржи ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рзЗ ржирждрзБржи ржЕржмрж╕рзНржерж╛ $s_1$ ржУ ржкрзБрж░рж╕рзНржХрж╛рж░ $r_0$ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝**:

   $r_0$ = $R(s_0, a_0, s_1)$
5. **Q-ржорж╛ржи ржЖржкржбрзЗржЯ ржХрж░рж╛ рж╣ржпрж╝ (Bellman Equation ржЕржирзБржпрж╛ржпрж╝рзА)**:
   
   $Q(s_0, a_0)$ = $(1 - \alpha) Q(s_0, a_0) + \alpha \left( r_0 + \gamma \max_{a \in A} Q(s_1, a) \right)$

   - ржПржЦрж╛ржирзЗ, $\alpha$ рж╣рж▓ **рж▓рж╛рж░рзНржирж┐ржВ рж░рзЗржЯ**, ржпрж╛ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ ржкрзБрж░рзЛржирзЛ ржорж╛ржи ржУ ржирждрзБржи ржорж╛ржирзЗрж░ weight ред
   - $\gamma$ рж╣рж▓ **ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржлрзНржпрж╛ржХрзНржЯрж░**, ржпрж╛ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред
6. **ржпржжрж┐ $s_1$ ржЯрж╛рж░рзНржорж┐ржирж╛рж▓ рж╕рзНржЯрзЗржЯ ржирж╛ рж╣ржпрж╝**, рждрж╛рж╣рж▓рзЗ ржзрж╛ржк рзй ржерзЗржХрзЗ ржкрзБржирж░рж╛ржмрзГрждрзНрждрж┐ ржХрж░рж╛ рж╣ржпрж╝ред

<br>

---


---

<br>

---

---

<br>

# **Q-learning with Epsilon-Greedy**:

`ржЫржмрж┐рждрзЗ Q-learning ржПрж░ ржорзМрж▓рж┐ржХ ржзрж╛ржкржЧрзБрж▓рзЛ ржжрзЗржЦрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ, <br>
ржПржЦрж╛ржирзЗ, ржкрзНрж░ржержо ржзрж╛ржкрзЗ ржЖржорж░рж╛  $Q(s, a)$ ржПрж░ ржЬржирзНржп рж░тАНрзНржпрж╛ржирзНржбржо ржнрзНржпрж╛рж▓рзБ (рж╕ржм рж╢рзВржирзНржп) ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ рждрзГрждрзАрзЯ ржзрж╛ржкрзЗ ржЖржорж░рж╛ рж╕рзЗржЗ (рж╕ржм рж╢рзВржирзНржп) ржнрзНржпрж╛рж▓рзБ ржЧрзБрж▓рзЛ ржерзЗржХрзЗ, $Q(s_0, a)$ ржорж╛ржи ржХрж░рж┐ ред ржПржЦржи, ржЖржорж░рж╛ ржпржжрж┐ рж░тАНрзНржпрж╛ржирзНржбржо ржнрзНржпрж╛рж▓рзБ (рж╕ржм рж╢рзВржирзНржп) ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржирж╛ ржХрж░рзЗ ржЖржорж░рж╛ ржПржХржЯрж╛ random action ржирзЗржЗ, рждрж╛рж╣рж▓рзЗ ржЖржорж░рж╛ рж╕рзЗржЗржЯрж╛ржХрзЗ ржмрж▓рждрзЗржЫрж┐ Epsilon-Greedy ржпрзЛржЧ ржХрж░рж╛ ред`

<br>

![image](../img/img09.png)

<br>



