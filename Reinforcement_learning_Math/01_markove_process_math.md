
![image](../img/img02.png)

<br>
<br>

![imgage](../img/img03.png)

<br>
<br>

![image_image](../img/img01.png)


### **Markov Decision Process: Dynamics & Discount (ржмрзНржпрж╛ржЦрзНржпрж╛)**  

ржПржЗ ржЫржмрж┐ржЯрж┐ **Markov Decision Process (MDP)**-ржПрж░ **ржбрж╛рзЯржирж╛ржорж┐ржХрзНрж╕ ржПржмржВ ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржлрзНржпрж╛ржХрзНржЯрж░ (╬│)** ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░ржЫрзЗред  


### **ржзрж╛ржкржЧрзБрж▓рзЛ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг**  

1я╕ПтГг **рж╢рзБрж░рзБ (Initial State):**  
   - ржПржЬрзЗржирзНржЯ (Agent) **sтВА** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ рж╕рзНржЯрзЗржЯ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ, ржпрж╛ **S (State Space)**-ржПрж░ ржЕржВрж╢ред  

2я╕ПтГг **ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рж░рзНржмрж╛ржЪржи (Choosing an Action):**  
   - ржПржЬрзЗржирзНржЯ **aтВА** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗрзЯ, ржпрж╛ **A (Action Space)**-ржПрж░ ржЕржВрж╢ред  

3я╕ПтГг **ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржкрж░рж┐ржмрж░рзНрждржи (State Transition):**  
   - ржПржЬрзЗржирзНржЯ **sтВБ** ржирж╛ржорзЗрж░ ржПржХржЯрж┐ ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржЪрж▓рзЗ ржпрж╛рзЯ, ржпрж╛ **S**-ржПрж░ ржоржзрзНржпрзЗржЗ ржЖржЫрзЗред  
   - ржПржЗ ржкрж░рж┐ржмрж░рзНрждржи ржПржХржЯрж┐ **Transition Probability** ржжрзНржмрж╛рж░рж╛ ржирж┐рж░рзНржзрж╛рж░рж┐ржд рж╣рзЯ:  
     T(sтВА, aтВА, sтВБ)

   - ржирждрзБржи рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУрзЯрж╛рж░ рж╕ржорзЯ ржПржЬрзЗржирзНржЯ ржХрж┐ржЫрзБ **ржкрзБрж░рж╕рзНржХрж╛рж░ (Reward)** ржкрж╛рзЯ:  
     R(sтВА, aтВА, sтВБ)


4я╕ПтГг **ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛ ржЪрж▓рждрзЗ ржерж╛ржХрзЗ (Repeat Until Termination):**  
   - ржПржЗ ржЪржХрзНрж░ (loop) ржЪрж▓рждрзЗ ржерж╛ржХрзЗ ржпрждржХрзНрж╖ржг ржирж╛ ржПржЬрзЗржирзНржЯ ржПржХржЯрж┐ **Terminal State**-ржП ржкрзМржБржЫрзЗ ржмрж╛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ рж╕ржВржЦрзНржпржХ ржзрж╛ржк **T** ржкрзВрж░рзНржг рж╣рзЯред  



### **ржЧрзЛрж▓ (Goal of the Agent)**  
ржПржЬрзЗржирзНржЯ ржПржоржиржнрж╛ржмрзЗ **aтВА, aтВБ, aтВВ, ...** ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржмрзЗ, ржпрж╛рждрзЗ рждрж╛рж░ ржорзЛржЯ ржкрзБрж░рж╕рзНржХрж╛рж░ рж╕рж░рзНржмрзЛржЪрзНржЪ рж╣рзЯ:  

$\sum_{j \geq 0} R(s_j, a_j, s_{j+1})$
ржЕрж░рзНржерж╛рзО, ржПржЬрзЗржирзНржЯ ржнржмрж┐рж╖рзНржпрждрзЗ ржпржд ржмрзЗрж╢рж┐ ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржмрзЗ, рждрж╛рж░ ржЬржирзНржп рждржд ржнрж╛рж▓рзЛ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирж┐рждрзЗ ржЪрж╛рзЯред  



### **рж╕ржорж╕рзНржпрж╛ (Problem of Infinite Sum)**  
- ржПржЦрж╛ржирзЗ ржПржХржЯрж┐ рж╕ржорж╕рзНржпрж╛ рж╣рж▓рзЛ, ржПржЗ **ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржпрзЛржЧржлрж▓ ржЕрж╕рзАржо (infinite sum) рж╣рждрзЗ ржкрж╛рж░рзЗ**ред  
- рждрж╛ржЗ, ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ **ржХржорж┐рзЯрзЗ ржЖржирж╛ (discounted)** рж╣рзЯред  
- ржПржХрзНрж╖рзЗрждрзНрж░рзЗ **Discount Factor (╬│)** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ, ржпрж╛ **0 ржерзЗржХрзЗ 1** ржПрж░ ржоржзрзНржпрзЗ ржерж╛ржХрзЗред  


### **Discounted Total Reward ржлрж░рзНржорзБрж▓рж╛**  

$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

ржПржЦрж╛ржирзЗ,  
тЬЕ **╬│ (Discount Factor)** тЖТ 0 ржПржмржВ 1 ржПрж░ ржоржзрзНржпрзЗ ржПржХржЯрж┐ ржорж╛ржи, ржпрж╛ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм ржарж┐ржХ ржХрж░рзЗред  
тЬЕ **╬│ ржмрзЗрж╢рж┐ рж╣рж▓рзЗ (╬│ тЙИ 1)** тЖТ ржПржЬрзЗржирзНржЯ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржмрзЗрж╢рж┐ ржЧрзБрж░рзБрждрзНржм ржжрзЗржмрзЗред  
тЬЕ **╬│ ржХржо рж╣рж▓рзЗ (╬│ тЙИ 0)** тЖТ ржПржЬрзЗржирзНржЯ рж╢рзБржзрзБржорж╛рждрзНрж░ рждрж╛рзОржХрзНрж╖ржгрж┐ржХ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржЧрзБрж░рзБрждрзНржм ржжрзЗржмрзЗред  


### **ржЙржжрж╛рж╣рж░ржг (Example)**  
ржзрж░рж┐, ржПржХржЬржи рж╕рзНржЯрзБржбрзЗржирзНржЯ ржкрзНрж░рждрж┐ржжрж┐ржи ржкрзЬрж╛рж╢рзЛржирж╛ ржХрж░рж▓рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗ ржнрж╛рж▓рзЛ ржЪрж╛ржХрж░рж┐ ржкрж╛ржмрзЗ (High Reward in Future)**ред  
- ржпржжрж┐ рж╕рзЗ **рж╢рзБржзрзБ ржмрж░рзНрждржорж╛ржи ржЖрж░рж╛ржо ржжрзЗржЦрзЗ (╬│ тЙИ 0)**, рждрж╛рж╣рж▓рзЗ ржкрзЬрж╛рж╢рзЛржирж╛рзЯ ржлрзЛржХрж╛рж╕ ржХрж░ржмрзЗ ржирж╛ред  
- ржпржжрж┐ рж╕рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗрж░ рж▓рж╛ржн ржжрзЗржЦрзЗ (╬│ тЙИ 1)**, рждрж╛рж╣рж▓рзЗ ржжрзАрж░рзНржШржорзЗрзЯрж╛ржжрзА рж▓рж╛ржнрзЗрж░ ржЬржирзНржп ржкрж░рж┐рж╢рзНрж░ржо ржХрж░ржмрзЗред  

### **рж╕ржВржХрзНрж╖рзЗржкрзЗ:**  
тЬЕ **Markov Decision Process (MDP)** ржПржЬрзЗржирзНржЯрзЗрж░ **ржЕрзНржпрж╛ржХрж╢ржи ржПржмржВ рж░рж┐ржУрзЯрж╛рж░рзНржб** ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред  
тЬЕ **Transition Probability** ржмрж▓рзЗ ржжрзЗрзЯ, ржПржХ рж╕рзНржЯрзЗржЯ ржерзЗржХрзЗ ржЕржирзНржп рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУрзЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ред  
тЬЕ **Discount Factor (╬│)** ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред  
тЬЕ **╬│ тЙИ 1 рж╣рж▓рзЗ** ржПржЬрзЗржирзНржЯ ржжрзАрж░рзНржШржорзЗрзЯрж╛ржжрзА рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирзЗрзЯ, **╬│ тЙИ 0 рж╣рж▓рзЗ** рждрж╛рзОржХрзНрж╖ржгрж┐ржХ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржЧрзБрж░рзБрждрзНржм ржжрзЗрзЯред  

### **╬│ (ржЧрж╛ржорж╛) ржПржмржВ j ржХрзА?**  

#### **╬│ (ржЧрж╛ржорж╛) ржХрзА?**  
ржЧрж╛ржорж╛ $\gamma$ рж╣рж▓ **discount factor** (ржЫрж╛ржбрж╝рзЗрж░ рж╣рж╛рж░), ржпрж╛ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржмрж░рзНрждржорж╛ржирзЗрж░ рждрзБрж▓ржирж╛ржпрж╝ ржХрждржЯрж╛ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣ржмрзЗред  

- **$\gamma$ ржПрж░ ржорж╛ржи 0 ржерзЗржХрзЗ 1 ржПрж░ ржоржзрзНржпрзЗ ржерж╛ржХрзЗ $0 \leq \gamma \leq 1$**  
- ржпржжрж┐ **$\gamma$ ржмрзЗрж╢рж┐ рж╣ржпрж╝ $\gamma \approx 1$**, рждрж╛рж╣рж▓рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржмрж░рзНрждржорж╛ржирзЗрж░ ржорждрзЛржЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж░рж╛ рж╣ржпрж╝ред  
- ржпржжрж┐ **$\gamma$ ржХржо рж╣ржпрж╝ $\gamma \approx 0$**, рждрж╛рж╣рж▓рзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж░рж╛ рж╣ржпрж╝ ржПржмржВ рждрж╛ ржжрзНрж░рзБржд рж╣рж╛рж░рж┐ржпрж╝рзЗ ржпрж╛ржпрж╝ред  


#### **j ржХрзА?**  
**j рж╣рж▓рзЛ ржЯрж╛ржЗржо рж╕рзНржЯрзЗржк (Time Step) ржмрж╛ ржкрж░рзНржпрж╛ржпрж╝**ред ржПржЯрж┐ ржирж┐рж░рзНржжрзЗрж╢ ржХрж░рзЗ **ржХржд ржиржорзНржмрж░ рж╕ржоржпрж╝рзЗ ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУржпрж╝рж╛ рж╣ржЪрзНржЫрзЗ**ред  

- j = 0 тЖТ ржкрзНрж░ржержо рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ (immediate reward) тЖТ **$\gamma^0 = 1$** (ржХрзЛржи ржЫрж╛ржбрж╝ ржирзЗржЗ)  
- j = 1  тЖТ ржкрж░рзЗрж░ рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^1$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- j = 2  тЖТ рждрж╛рж░ ржкрж░рзЗрж░ рж╕ржоржпрж╝рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^2$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- j = 3  тЖТ ржЖрж░ржУ ржкрж░рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ тЖТ **$\gamma^3$** ржжрзНржмрж╛рж░рж╛ ржЫрж╛ржбрж╝ ржкрзНрж░рж╛ржкрзНржд  
- ржПржЗржнрж╛ржмрзЗ **ржкрзНрж░рждрзНржпрзЗржХ ржзрж╛ржкрзЗ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржорзВрж▓рзНржп ржХржорждрзЗ ржерж╛ржХрзЗ**  


#### **ржХрзЗржи ╬│^j ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣ржпрж╝?**  
ржЖржорж░рж╛ ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржХрж░рж╛ ржЯрзЛржЯрж╛рж▓ рж░рж┐ржУржпрж╝рж╛рж░рзНржб (Discounted Total Reward) рж╣рж┐рж╕рж╛ржм ржХрж░рж┐:  


$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

ржПржЦрж╛ржирзЗ:  
тЬЕ **$\gamma^j$ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ржХрзЗ ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржЫрзЛржЯ ржХрж░рзЗ ржжрзЗржпрж╝**, ржпрж╛рждрзЗ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЕржмржжрж╛ржи ржзрзАрж░рзЗ ржзрзАрж░рзЗ ржХржорждрзЗ ржерж╛ржХрзЗред  
тЬЕ **j ржпржд ржмржбрж╝ рж╣ржпрж╝, $\gamma^j$ рждржд ржЫрзЛржЯ рж╣ржпрж╝**, ржЕрж░рзНржерж╛рзО ржЕржирзЗржХ ржжрзВрж░рзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░ ржХржо ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг рж╣ржпрж╝рзЗ ржпрж╛ржпрж╝ред  
тЬЕ **ржПржЯрж┐ ржЗржиржлрж┐ржирж┐ржЯ (ржЕрж╕рзАржо) рж░рж┐ржУржпрж╝рж╛рж░рзНржб ржЕрзНржпрж╛ржХрзБржорзБрж▓рзЗрж╢ржи рж░рзЛржз ржХрж░рзЗ**, ржпрж╛рждрзЗ рж░рж┐ржУржпрж╝рж╛рж░рзНржбрзЗрж░ рж╣рж┐рж╕рж╛ржм рж╕рзАржорж┐ржд ржерж╛ржХрзЗред  

---

#### **ржЙржжрж╛рж╣рж░ржг**  
ржзрж░рзБржи, ржкрзНрж░рждрж┐ржЯрж┐ ржзрж╛ржкрзЗ ржПржЬрзЗржирзНржЯ **рззрзж ржкржпрж╝рзЗржирзНржЯ** ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржЪрзНржЫрзЗ, ржПржмржВ **ржЧрж╛ржорж╛ $\gamma$ = рзж.рзп**ред рждрж╛рж╣рж▓рзЗ:  

$R_{total} = 10 \times (0.9^0) + 10 \times (0.9^1) + 10 \times (0.9^2) + 10 \times (0.9^3)$ + ...

= 10 + 9 + 8.1 + 7.29 + 6.56 + ...


ржПржЦрж╛ржирзЗ ржжрзЗржЦрж╛ ржпрж╛ржЪрзНржЫрзЗ ржпрзЗ, **ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржкрж░рж┐ржорж╛ржг ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ ржХржоржЫрзЗ ржХрж╛рж░ржг $\gamma^j$ ржЫрзЛржЯ рж╣ржпрж╝рзЗ ржпрж╛ржЪрзНржЫрзЗ**ред

<br>

---

<br>

---

<br>

### **ржкрж▓рж┐рж╕рж┐ $\pi$ ржХрзАржнрж╛ржмрзЗ ржПрж▓рзЛ ржПржмржВ ржХрзЗржи ржжрж░ржХрж╛рж░?**  

#### **ЁЯУМ рзз. ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ ржХрзА ржЫрж┐рж▓ ржПржмржВ ржПрждрзЗ ржХрзА ржмрзЛржЭрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ?**  
ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржЯрж┐ ржЫрж┐рж▓тАФ  


$\sum_{j \geq 0} \gamma^j R(s_j, a_j, s_{j+1})$

- **ржПржЗ ржлрж░рзНржорзБрж▓рж╛ ржмрж▓рзЗ, ржпржжрж┐ ржЖржорж░рж╛ \(s_0\) ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рж┐, рждрж╛рж╣рж▓рзЗ ржорзЛржЯ ржХржд ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржЖрж╢рж╛ ржХрж░рж╛ ржпрж╛ржпрж╝ред**  

ржХрж┐ржирзНрждрзБ рж╕ржорж╕рзНржпрж╛ рж╣рж▓рзЛ, **ржПржЬрзЗржирзНржЯ ржХрзАржнрж╛ржмрзЗ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржмрзЗ, рж╕рзЗржЯрж┐ ржПржЦрж╛ржирзЗ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣ржпрж╝ржирж┐!**  


#### **ЁЯУМ рзи. рждрж╛рж╣рж▓рзЗ ржкрж▓рж┐рж╕рж┐рж░ $\pi$ ржжрж░ржХрж╛рж░ ржХрзЗржи?**  
ржЖржорж░рж╛ ржЬрж╛ржирж┐, **reinforcement learning-ржПрж░ ржорзВрж▓ ржХрж╛ржЬ рж╣рж▓рзЛ рж╕ржарж┐ржХ ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржУрзЯрж╛, ржпрж╛рждрзЗ рж╕рж░рзНржмрзЛржЪрзНржЪ ржкрзБрж░рж╕рзНржХрж╛рж░ ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржпрж╝ред** ржХрж┐ржирзНрждрзБ ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржпрж╝ action $a_j$ ржХрзАржнрж╛ржмрзЗ ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗ, рждрж╛ ржмрж▓рж╛ рж╣рзЯржирж┐ред  

ржПржЬрзЗржирзНржЯржХрзЗ ржпржжрж┐ **ржПржХржЯрж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржирж┐ржпрж╝ржо ржЕржирзБрж╕рж╛рж░рзЗ ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рждрзЗ ржмрж▓рж╛ рж╣ржпрж╝**, рждрж╛рж╣рж▓рзЗ рж╕рзЗржЯрж╛ржЗ **ржкрж▓рж┐рж╕рж┐ (Policy, $\pi$**!  


$\pi : S \rightarrow A$

ржорж╛ржирзЗ, **ржкрж▓рж┐рж╕рж┐ ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи, ржпрж╛ ржмрж▓рзЗ ржжрзЗржпрж╝ ржкрзНрж░рждрж┐ржЯрж┐ state-ржП ржХрзЛржи action ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗред**  

ржпрзЗржоржиржГ  
- ржпржжрж┐ **s = ржХрзНрж╖рзБржзрж╛рж░рзНржд (hungry)** рж╣рзЯ, рждрж╛рж╣рж▓рзЗ **$\pi(s)$ = ржЦрж╛ржУ (eat)**ред  
- ржпржжрж┐ **s = ржЯрзНрж░рж╛ржлрж┐ржХ рж▓рж╛ржЗржЯ рж▓рж╛рж▓ (red light)** рж╣рзЯ, рждрж╛рж╣рж▓рзЗ **$\pi(s)$ = ржжрж╛ржБрзЬрж╛ржУ (stop)**ред  

ЁЯУМ **рж╕рзЛржЬрж╛ ржнрж╛рж╖рж╛ржпрж╝, ржкрж▓рж┐рж╕рж┐ рж╣рж▓рзЛ ржПржоржи ржПржХржЯрж┐ ржирж┐ржпрж╝ржо, ржпрж╛ state ржЕржирзБржпрж╛ржпрж╝рзА action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред**  


#### **ЁЯУМ рзй. ржкрж▓рж┐рж╕рж┐рж░ ржЬржирзНржп ржирждрзБржи ржлрж░рзНржорзБрж▓рж╛ ржХрзА?**  

ржпрзЗрж╣рзЗрждрзБ ржкрж▓рж┐рж╕рж┐ **ржкрзНрж░рждрж┐ржЯрж┐ state-ржПрж░ ржЬржирзНржп action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗ**, рждрж╛ржЗ ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржХрзЗ ржЖржкржбрзЗржЯ ржХрж░рзЗ **policy-based equation** ржмрж╛ржирж╛ржирзЛ рж╣ржпрж╝тАФ  


$V_{\pi}(s) = E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) | s_0 = s \right]$

ржПржЦрж╛ржирзЗ,  
- **$\pi(s_j)$** тЖТ ржПржЦржи ржЖржорж░рж╛ ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ action $a_j$ рж▓рж┐ржЦржЫрж┐ ржирж╛, ржХрж╛рж░ржг **ржкрж▓рж┐рж╕рж┐ ржЖржЧрзЗ ржерзЗржХрзЗржЗ ржмрж▓рзЗ ржжрж┐ржЪрзНржЫрзЗ, ржкрзНрж░рждрж┐ржЯрж┐ state-ржП ржХрзЛржи action ржирж┐рждрзЗ рж╣ржмрзЗ!**  
- **$V_{\pi}(s)$** тЖТ ржПрж░ ржорж╛ржирзЗ, **ржПржЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ policy $\pi$ ржЕржирзБрж╕рж░ржг ржХрж░рж▓рзЗ, state (s) ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ ржорзЛржЯ ржХржд ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржмрзЗ**ред  

ЁЯУМ **ржорзВрж▓ ржкрж╛рж░рзНржержХрзНржп:**  
- ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржпрж╝ action ржЫрж┐рж▓ **$a_j$**, ржпрзЗржЯрж┐ ржЖржорж░рж╛ ржирж┐ржЬрзЗрж░рж╛ ржарж┐ржХ ржХрж░рж┐ржирж┐ред  
- ржПржЦржи **$a_j = \pi(s_j)$**, ржорж╛ржирзЗ **policy рж╕рзНржмржпрж╝ржВржХрзНрж░рж┐ржпрж╝ржнрж╛ржмрзЗ action ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░ржЫрзЗред**  
 
<br>

# `## Conditional Expectation (E) vs Conditional Probability (P):`

`In the formula:`

P(A | B) = $\text{B ржШржЯрзЗржЫрзЗ, рждржЦржи A ржШржЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛}$
ржПржЦржи, ржЖржорж╛ржжрзЗрж░ ржлрж░рзНржорзБрж▓рж╛ржЯрж┐:  

$E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$

ржПржЦрж╛ржирзЗ, **" $\mid s_0 = s$ "** ржЕржВрж╢ржЯрж╛ ржЖрж╕рж▓рзЗ рж╢рж░рзНржд (condition) ржмрзЛржЭрж╛ржЪрзНржЫрзЗред рждрж╛ржЗ ржЖржорж░рж╛ ржмрж▓рждрзЗ ржкрж╛рж░рж┐,  

ЁЯУМ **ржЖржорж░рж╛ ржПржоржи ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи (Expected Value) ржмрзЗрж░ ржХрж░ржЫрж┐ ржпрзЗржЦрж╛ржирзЗ $s_0 = s $ ржПржЗ рж╢рж░рзНрждржЯрж┐ ржорзЗржирзЗ ржирзЗржУржпрж╝рж╛ рж╣ржпрж╝рзЗржЫрзЗред**  


# **ЁЯУМ рждрж╛рж╣рж▓рзЗ, ржПржЦрж╛ржирзЗ рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛рж░ (Conditional Probability) рж╕ржорзНржкрж░рзНржХ ржХрзА?**  

## **тЬЕ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛ (Conditional Expectation) ржмржирж╛ржо рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛ (Conditional Probability)**
 
Reinforcement Learning (RL)-ржП ржЖржорж░рж╛ ржорзВрж▓ржд **рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж┐, ржХрж┐ржирзНрждрзБ ржПрж░ ржкрж┐ржЫржирзЗ **рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛рж░** ржзрж╛рж░ржгрж╛ рж░рзЯрзЗржЫрзЗред  

### **ЁЯФ╣ рж╢рж░рзНрждрж╛ржзрзАржи рж╕ржорзНржнрж╛ржмржирж╛ (Conditional Probability)**

P(A | B) = $\frac{P(A \cap B)}{P(B)}$
ЁЯФ╣ ржПржЯрж┐ ржмрзЛржЭрж╛рзЯ ржпрзЗ, **B ржШржЯржирж╛ ржШржЯрзЗ ржЧрзЗржЫрзЗ, рждржЦржи A ржШржЯрж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ ржХржд?**  
ЁЯФ╣ Reinforcement Learning-ржП **ржкрж░ржмрж░рзНрждрзА рж╕рзНржЯрзЗржЯрзЗ ржпрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛** ржмрзЛржЭрж╛рждрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред  
ЁЯФ╣ ржЙржжрж╛рж╣рж░ржг:  
   - рждрзБржорж┐ ржпржжрж┐ ржПржЦржи **$s_t$** рж╕рзНржЯрзЗржЯрзЗ ржерж╛ржХрзЛ ржПржмржВ **$a_t$** ржЕрзНржпрж╛ржХрж╢ржи ржирж╛ржУ, рждрж╛рж╣рж▓рзЗ **ржкрж░ржмрж░рзНрждрзА рж╕рзНржЯрзЗржЯрзЗ $s_{t+1}$ ржпрж╛ржУржпрж╝рж╛рж░ рж╕ржорзНржнрж╛ржмржирж╛ ржХржд?**  
   - ржЕрж░рзНржерж╛рзО, $P(s_{t+1} | s_t, a_t)$


### **ЁЯФ╣ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛ (Conditional Expectation)**

$E[X | Y] = \sum_{x} x P(X = x | Y)$
ЁЯФ╣ ржПржЯрж┐ ржмрж▓рзЗ ржпрзЗ, **Y ржШржЯрзЗ ржпрж╛ржУржпрж╝рж╛рж░ ржкрж░ X ржПрж░ ржЧрзЬ ржорж╛ржи (expected value) ржХржд?**  
ЁЯФ╣ Reinforcement Learning-ржП **ржнрзНржпрж╛рж▓рзБ ржлрж╛ржВрж╢ржи** ржирж┐рж░рзНржзрж╛рж░ржгрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯред  
ЁЯФ╣ ржЙржжрж╛рж╣рж░ржг:  
   - ржпржжрж┐ $s_0$ = s  ржШржЯрзЗ ржерж╛ржХрзЗ, рждрж╛рж╣рж▓рзЗ **ржнржмрж┐рж╖рзНржпрждрзЗ ржкрзНрж░рж╛ржкрзНржд рж╕ржорж╕рзНржд ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзЬ ржорж╛ржи** ржХрзА рж╣ржмрзЗ?  
   - ржЕрж░рзНржерж╛рзО,  
     
    $V_\pi(s) = E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$
   - ржПржЦрж╛ржирзЗ **$s_0 = s$** ржзрж░рзЗ ржирж┐рзЯрзЗ ржнржмрж┐рж╖рзНржпрзО ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи ржмрзЗрж░ ржХрж░рж╛ рж╣ржЪрзНржЫрзЗред  

## **тЬЕ Reinforcement Learning-ржП Policy ржирж┐рж░рзНржзрж╛рж░ржгрзЗрж░ рж╕рзВрждрзНрж░**
### **ЁЯУМ Policy $\pi$ ржорж╛ржирзЗ ржХрзА?**  
ЁЯФ╣ Policy рж╣рж▓рзЛ ржПржХржЯрж┐ ржлрж╛ржВрж╢ржи ржпрж╛ ржмрж▓рзЗ **"ржкрзНрж░рждрзНржпрзЗржХ рж╕рзНржЯрзЗржЯрзЗ ржХрзЛржи ржЕрзНржпрж╛ржХрж╢ржи ржирзЗржУржпрж╝рж╛ ржЙржЪрж┐ржд?"**  
ЁЯФ╣ ржЕрж░рзНржерж╛рзО,  

   $\pi : S \to A$
   **ржпрзЗржЦрж╛ржирзЗ,**  
   - S = рж╕рзНржЯрзЗржЯ рж╕рзНржкрзЗрж╕  
   - A = ржЕрзНржпрж╛ржХрж╢ржи рж╕рзНржкрзЗрж╕  


### **ЁЯУМ Policy ржирж┐рж░рзНржзрж╛рж░ржгрзЗрж░ ржлрж░рзНржорзБрж▓рж╛**
$V_\pi(s) = E \left[ \sum_{j \geq 0} \gamma^j R(s_j, \pi(s_j), s_{j+1}) \mid s_0 = s \right]$

ЁЯФ╣ **ржПржЦрж╛ржирзЗ:**  
   -  $V_\pi(s)$ = **рж╕рзНржЯрзЗржЯ s-ржП ржерж╛ржХрж╛рж░ ржорж╛ржи (value function)**  
   - **$E[\cdot]$** = **ржкрзНрж░рждрзНржпрж╛рж╢рж┐ржд ржорж╛ржи (Expectation)**  
   - **$\gamma^j$** = **ржбрж┐рж╕ржХрж╛ржЙржирзНржЯ ржлрзНржпрж╛ржХрзНржЯрж░** (ржнржмрж┐рж╖рзНржпрждрзЗрж░ рж░рж┐ржУржпрж╝рж╛рж░рзНржбрзЗрж░ ржЧрзБрж░рзБрждрзНржм ржХржорж╛ржирзЛрж░ ржЬржирзНржп)  
   - **$R(s_j, \pi(s_j), s_{j+1})$** = **ржмрж░рзНрждржорж╛ржи рж░рж┐ржУржпрж╝рж╛рж░рзНржб**  
   - **$\pi(s_j)$** = **ржкрж▓рж┐рж╕рж┐ ржЕржирзБржпрж╛рзЯрзА ржЕрзНржпрж╛ржХрж╢ржи ржирж┐рж░рзНржмрж╛ржЪржи**  

тЬЕ **ржПржЯрж┐ ржЖрж╕рж▓рзЗ рж╢рж░рзНрждрж╛ржзрзАржи ржкрзНрж░рждрзНржпрж╛рж╢рж╛** (Conditional Expectation), ржХрж╛рж░ржг ржЖржорж░рж╛ $s_0$ = s ржзрж░рзЗ ржирж┐рзЯрзЗ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржкрзБрж░рж╕рзНржХрж╛рж░рзЗрж░ ржЧрзЬ ржорж╛ржи рж╣рж┐рж╕рж╛ржм ржХрж░ржЫрж┐ред  


#### **ЁЯУМ рзк. Reinforcement Learning-ржП Policy ржХрзЗржи ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг?**  
тЬЕ **Policy $\pi$ ржЫрж╛рзЬрж╛ ржПржЬрзЗржирзНржЯ ржЬрж╛ржиржмрзЗ ржирж╛, ржХрзЛржи action ржирж┐рж▓рзЗ рж╕рж░рзНржмрзЛржЪрзНржЪ reward ржкрж╛ржУрзЯрж╛ ржпрж╛ржмрзЗред**  
тЬЕ **ржЖржорж╛ржжрзЗрж░ ржорзВрж▓ рж▓ржХрзНрж╖рзНржп рж╣рж▓рзЛ optimal policy ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛, ржпрж╛ рж╕рж░рзНржмрзЛржЪрзНржЪ ржкрзБрж░рж╕рзНржХрж╛рж░ ржжрзЗржмрзЗред**  
тЬЕ **Policy ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж▓рзЗ, ржПржЬрзЗржирзНржЯ рж╕рж╣ржЬрзЗржЗ рж╕рж┐ржжрзНржзрж╛ржирзНржд ржирж┐рждрзЗ ржкрж╛рж░рзЗ ржПржмржВ environment ржерзЗржХрзЗ рж╢рзЗржЦрж╛рж░ ржХрзНрж╖ржорждрж╛ ржкрж╛ржпрж╝ред**  


### **ЁЯУМ рж╕ржВржХрзНрж╖рзЗржкрзЗ:**  
тЬЕ **ржЖржЧрзЗрж░ ржлрж░рзНржорзБрж▓рж╛** тЖТ рж╢рзБржзрзБржорж╛рждрзНрж░ reward ржПрж░ ржзрж╛рж░ржгрж╛ ржжрзЗржпрж╝, ржХрж┐ржирзНрждрзБ action ржХрзАржнрж╛ржмрзЗ ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗ рждрж╛ ржмрж▓рзЗ ржирж╛ред  
тЬЕ **Policy $\pi$** тЖТ ржмрж▓рзЗ ржжрзЗржпрж╝, **state-ржПрж░ ржнрж┐рждрзНрждрж┐рждрзЗ ржХрзА action ржирзЗржУржпрж╝рж╛ рж╣ржмрзЗред**  
тЬЕ **Policy Equation** тЖТ **ржПржЗ policy ржЕржирзБрж╕рж░ржг ржХрж░рж▓рзЗ ржорзЛржЯ ржХржд ржкрзБрж░рж╕рзНржХрж╛рж░ (reward) ржкрж╛ржУржпрж╝рж╛ ржпрж╛ржмрзЗ рждрж╛ ржирж┐рж░рзНржгржпрж╝ ржХрж░рзЗред**  
тЬЕ **ржЧрзЛрж▓ (Goal)** тЖТ **ржПржХржЯрж┐ optimal policy ржЦрзБржБржЬрзЗ ржмрзЗрж░ ржХрж░рж╛, ржпрж╛ рж╕рж░рзНржмрзЛржЪрзНржЪ ржкрзБрж░рж╕рзНржХрж╛рж░ ржжрзЗржмрзЗред**  

<br>

![imgage](../img/img04.png)

<br>
