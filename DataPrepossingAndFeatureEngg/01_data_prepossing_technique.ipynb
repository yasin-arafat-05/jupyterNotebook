{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preproceing and Feature Engineering\n",
    "\n",
    "### [Video Link](https://www.youtube.com/watch?v=3imSHVySLRc&list=PLfP3JxW-T70GR0w3zVzG7tgIFI14FZxaj&index=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: \n",
    "Data preprocessing is a process to convert raw data into meaningful data using different techniques.\n",
    "<br>\n",
    "\n",
    "### Why is Data Preprocessing Important\n",
    "### Data in the real World is dirty.\n",
    "- Incomplete\n",
    "- Noisy\n",
    "- Inconsistent\n",
    "- Duplicate\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### To train our ML algorithrm(Learn LIke A Kids) We need to convert this data into quality data. Those data have\n",
    "\n",
    "- Accuracy \n",
    "- Completeness\n",
    "- Consistency \n",
    "- Believability\n",
    "- Interpretability\n",
    "\n",
    "--- \n",
    "# Explanation:\n",
    "1. **Accuracy**:\n",
    "   - **Definition**: Accuracy refers to how close the data values are to the true values or the actual state of affairs.\n",
    "   - **Importance in ML**: In machine learning, accurate data ensures that the model learns from reliable information, leading to more precise predictions or classifications.\n",
    "   - **Example**: If a dataset records the heights of individuals, accurate data would mean that the recorded heights closely match the actual heights of those individuals.\n",
    "\n",
    "2. **Completeness**:\n",
    "   - **Definition**: Completeness refers to the extent to which all required data is present in the dataset without any missing values.\n",
    "   - **Importance in ML**: Complete data ensures that the model has access to all necessary information for learning patterns and making predictions.\n",
    "   - **Example**: In a dataset recording customer information, completeness would mean that there are no missing entries for essential fields like name, age, or contact information.\n",
    "\n",
    "3. **Consistency**:\n",
    "   - **Definition**: Consistency in data refers to uniformity and coherence across the dataset. It means that data values are presented in a standardized manner without contradictions or discrepancies.\n",
    "   - **Importance in ML**: In machine learning, consistent data ensures that the patterns and relationships learned by the model are reliable and accurate. Inconsistent data could lead to biased or incorrect conclusions.\n",
    "   - **Example**: Suppose you have a dataset of customer addresses where some entries use abbreviated state names (e.g., \"CA\" for California) while others use the full state name (\"California\"). Achieving consistency would involve standardizing the format so that all addresses use the same representation for states.\n",
    "\n",
    "4. **Believability**:\n",
    "   - **Definition**: Believability refers to the trustworthiness and reliability of the data. It involves assessing the credibility of the information based on its source, accuracy, and relevance.\n",
    "   - **Importance in ML**: Believable data is crucial for building trustworthy machine learning models. If the data used for training is not credible, the resulting model's predictions or insights may be inaccurate or misleading.\n",
    "   - **Example**: Consider a dataset of medical records collected from reputable hospitals and clinics versus one obtained from unknown sources online. The former would generally be more believable due to the credibility of the institutions involved.\n",
    "\n",
    "5. **Interpretability**:\n",
    "   - **Definition**: Interpretability refers to the ease with which data can be understood and analyzed by humans. It involves presenting data in a clear and understandable format, often with proper documentation.\n",
    "   - **Importance in ML**: In machine learning, interpretability is crucial for stakeholders to understand how models make predictions or classifications. Transparent models and interpretable data help users trust and validate the model's outputs.\n",
    "   - **Example**: If you're analyzing a complex dataset with numerous variables, ensuring interpretability might involve providing clear descriptions and explanations of each variable's meaning and significance, as well as visualizations that aid in understanding the data's underlying patterns.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Major setps in Data Preprocessing : \n",
    "- Data Cleaning\n",
    "- Data Integration\n",
    "- Data Reduction\n",
    "- Data Transformation\n",
    "-Data Discretization\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Data Cleaning?\n",
    "\n",
    "- Data Cleaning means fill in missing values, smooth out noise while identifying outliers, and corrext inconsistencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "      Name   Age   Salary\n",
      "0     John  25.0  50000.0\n",
      "1    Alice  28.0  60000.0\n",
      "2      Bob   NaN  75000.0\n",
      "3    Alice  28.0      NaN\n",
      "4  Charlie  35.0  90000.0\n",
      "5    David  30.0  80000.0\n",
      "6     John  25.0  50000.0\n",
      "\n",
      "DataFrame after handling missing values:\n",
      "      Name   Age   Salary\n",
      "0     John  25.0  50000.0\n",
      "1    Alice  28.0  60000.0\n",
      "4  Charlie  35.0  90000.0\n",
      "5    David  30.0  80000.0\n",
      "6     John  25.0  50000.0\n",
      "\n",
      "DataFrame after removing duplicates:\n",
      "      Name   Age   Salary\n",
      "0     John  25.0  50000.0\n",
      "1    Alice  28.0  60000.0\n",
      "4  Charlie  35.0  90000.0\n",
      "5    David  30.0  80000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with missing values and duplicate rows\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Alice', 'Charlie', 'David', 'John'],\n",
    "    'Age': [25, 28, None, 28, 35, 30, 25],\n",
    "    'Salary': [50000, 60000, 75000, None, 90000, 80000, 50000]\n",
    "}\n",
    "\n",
    "# Convert the data into pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Handling missing values\n",
    "df_cleaned = df.dropna()  # Remove rows with missing values\n",
    "\n",
    "# Display the DataFrame after handling missing values\n",
    "print(\"\\nDataFrame after handling missing values:\")\n",
    "print(df_cleaned)\n",
    "\n",
    "# Remove duplicate rows\n",
    "df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "# Display the DataFrame after removing duplicates\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Data Integration?\n",
    "- Data Integration is a technique to merges data from multiple sources \n",
    "into a coherent data store, such as a data warehouse.\n",
    "\n",
    "![Data Integration Example:](assets/Data_integration.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What is Data Reduction?\n",
    "- Data Reduction is a technique use to reduce the data size by aggregating, eliminating redundant features, or clustering, for instace.\n",
    "\n",
    "- We use Principal Component analysis(we will see it in ml) to the data reduction process\n",
    "\n",
    "![data Reduction!](assets/data_reduction.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between **Data Integration** and **Data Reduction**\n",
    "**Data Integration** and **Data Reduction** are two different concepts in the field of data management and analysis:\n",
    "\n",
    "1. **Data Integration:**\n",
    "   - **Definition:** Data integration is the process of combining data from different sources into a unified view. It involves merging data from disparate sources to provide a more comprehensive and cohesive understanding of the information.\n",
    "   \n",
    "<br>\n",
    "\n",
    "2. **Data Reduction:**\n",
    "   - **Definition:** Data reduction is the process of reducing the volume but producing the same or similar analytical results. It involves simplifying complex data sets while retaining the inherent information and characteristics needed for analysis.<br>\n",
    "\n",
    "   - **Purpose:** The primary purpose of data reduction is to reduce the size or complexity of a dataset without significantly sacrificing its analytical value. This is often done to improve efficiency in data storage, processing, and analysis, especially when dealing with large datasets.\n",
    "   \n",
    "\n",
    "In summary, data integration is about combining data from different sources into a unified view, whereas data reduction is about reducing the size or complexity of a dataset while preserving its analytical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Data Transformation?\n",
    "\n",
    "- Data Transformation means data are Transformed or consolidated into forms appropritate for ML model traning such as normilization may be applied where data are scaled to fall within a smaller range like 0.0 to 1.0.\n",
    "\n",
    "- Aggregation\n",
    "- Feature type conversion\n",
    "- Normalization\n",
    "- Attribute/feature construction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"assets/data_discretization.jpg\">\n",
    "\n",
    "<br>\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "<img src=\"DataPrepossingAndFeatureEngg/assets/data_discretization.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
